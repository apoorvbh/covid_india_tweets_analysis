{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"colab":{"name":"topic-modeling-bert-lda.ipynb","provenance":[],"collapsed_sections":["kwodVomHRZ-z","EWudO6zPRZ_4"]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"4bb6b376f0fb44b5938886267fb74728":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_700ab3aa4be94dcb89dff8cedbd0e279","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_efb38403471d412ea5d2c78a8b2df998","IPY_MODEL_8ec65b24b0414cc28b3a942ef32fe7b6"]}},"700ab3aa4be94dcb89dff8cedbd0e279":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"efb38403471d412ea5d2c78a8b2df998":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_898a166bdb2c465ba9a93dc0b749bf23","_dom_classes":[],"description":"Batches: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":78293,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":78293,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d6a04bd3795490198d0367f4f93e6ef"}},"8ec65b24b0414cc28b3a942ef32fe7b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f767e3e5cf40469a8a5cf74517d4b157","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 78293/78293 [15:38&lt;00:00, 83.41it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28339c149a5447718beaefb58d0f6282"}},"898a166bdb2c465ba9a93dc0b749bf23":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1d6a04bd3795490198d0367f4f93e6ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f767e3e5cf40469a8a5cf74517d4b157":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"28339c149a5447718beaefb58d0f6282":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"_kg_hide-output":true,"id":"FRzI1dqJi-3u","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1599115928776,"user_tz":-330,"elapsed":13792,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"4d76a6a9-b8a7-48a3-d9bf-01257b7ae8cb"},"source":["!pip install spacy-langdetect\n","!pip install language-detector\n","!pip install symspellpy\n","!pip install sentence-transformers\n","!pip install stop-words\n","!pip install pyLDAvis"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy-langdetect in /usr/local/lib/python3.6/dist-packages (0.1.2)\n","Requirement already satisfied: langdetect==1.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-langdetect) (1.0.7)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from spacy-langdetect) (3.6.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.15.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.4.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.9.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (49.6.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (20.1.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (8.4.0)\n","Requirement already satisfied: language-detector in /usr/local/lib/python3.6/dist-packages (5.0.2)\n","Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.7.0)\n","Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.18.5)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.3.5.1)\n","Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.6.0+cu101)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (0.1.91)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (0.0.43)\n","Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers) (0.8.1rc1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.2->sentence-transformers) (2.4.7)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->sentence-transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->sentence-transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->sentence-transformers) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.2->sentence-transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.2->sentence-transformers) (7.1.2)\n","Requirement already satisfied: stop-words in /usr/local/lib/python3.6/dist-packages (2018.7.23)\n","Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n","Requirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n","Requirement already satisfied: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.14)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.0.5)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (49.6.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.1.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.4.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1_SrWoOOmROr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1599116050514,"user_tz":-330,"elapsed":2869,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"e77aa549-82d3-4273-f8af-a794a92a37c3"},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"B-YLDAMYi-32","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116050517,"user_tz":-330,"elapsed":2172,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["#covid-papers-browser/models/scibert-nli/"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CtshQ3Zti-38","colab_type":"text"},"source":["# Topic Modeling with BERT, LDA, and Clustering\n","#### Latent Dirichlet Allocation (LDA) probabilistic topic assignment and pre-trained sentence embeddings from BERT/RoBERTa"]},{"cell_type":"markdown","metadata":{"id":"KeDciqbbi-39","colab_type":"text"},"source":["**Insired by:** \n","\n","Shoa, S. (2020). **Contextual Topic Identification: Identifying meaningful topics for sparse Steam reviews**. Medium. Available at: https://blog.insightdatascience.com/contextual-topic-identification-4291d256a032 [Accessed 25 Mar. 2020]."]},{"cell_type":"markdown","metadata":{"id":"AhKFrNeLi-4A","colab_type":"text"},"source":["## Model Deep Dive\n","\n","The author used: \n","\n","* LDA for probabilistic topic assignment vector.\n","* Bert for sentence embedding vector.\n","\n","1. Concatenated both LDA and Bert vectors with a weight hyperparameter to balance the relative importance of information from each source.\n","2. Used autoencoder to learn a lower dimensional latent space representation of the concatenated vector.\n","\n","* The assumption is that the concatendate vector shoul have a manifold shaep in the high dimensional space. \n","* USed clustering on the latent space representations to get topics. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"aFYu-e6yi-4A","colab_type":"text"},"source":["![Contextual Topic Identification model design](https://miro.medium.com/max/1410/1*OKCYnB-JbGq1NDwNSKd5Zw.png)"]},{"cell_type":"code","metadata":{"id":"QaWJi0ojjrJb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1599116323705,"user_tz":-330,"elapsed":2181,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"16dbcadf-c368-4de4-f4c1-c69e2ec7d806"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"TlfHvpy0i-4B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":645},"executionInfo":{"status":"ok","timestamp":1599116326394,"user_tz":-330,"elapsed":1200,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"32fe9ff2-d9cc-4bc5-cb69-5a3513477909"},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/content/drive/My Drive/tweets_analysis/cleaned_tweets_files'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output.\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-01.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-02.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-03.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-04.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-05.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-06.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-07.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-08.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-09.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-10.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-11.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-12.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-13.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-14.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-15.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-16.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-17.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-18.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-19.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-20.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-21.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-22.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-23.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-24.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-25.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-26.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-27.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-28.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-02-29.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-01.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-02.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-03.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-04.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-05.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-06.csv\n","/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/en_geo_2020-03-07.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LFWZzUt7i-4D","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116327427,"user_tz":-330,"elapsed":1600,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["#importing all dependencies.\n","import os\n","import json\n","import pandas as pd\n","from tqdm import tqdm\n","import numpy as np\n","from nltk.corpus import wordnet\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords \n","#from geotext import GeoText\n","#/kaggle/input"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GlSPH-aii-4G","colab_type":"text"},"source":["## Upload Data"]},{"cell_type":"markdown","metadata":{"id":"8cataZ_Ci-4G","colab_type":"text"},"source":["**Data pipeline (from development to deployment**)\n","\n","![Data pipeline (from development to deployment)](https://miro.medium.com/max/1348/1*Cdp4y1tfMxqoj96o6lUdFg.png)\n","\n","\n","Source:Shoa "]},{"cell_type":"code","metadata":{"id":"Nwwaabvji-4H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1599116333214,"user_tz":-330,"elapsed":4163,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"057640cd-6c22-4806-bb63-0d04653e652c"},"source":["#https://www.kaggle.com/jieyang0311/covid-19-topic-modeling-lda\n","#load library\n","import os\n","import pandas as pd\n","import numpy as np\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim import corpora, models\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","\n","import datetime\n","import time\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import nltk\n","import glob"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"InVJp-dai-4K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599116336124,"user_tz":-330,"elapsed":6426,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"c9a64bfe-ec6d-42d1-dc13-016aa4aa00f6"},"source":["tweets_df = pd.concat(map(pd.read_csv, glob.glob('/content/drive/My Drive/tweets_analysis/cleaned_tweets_files/*.csv')))\n","print(tweets_df.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(791511, 3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IgC_btSIknwW","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116674165,"user_tz":-330,"elapsed":343934,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["#Sentiment Analysis\n","from textblob import TextBlob\n","tweets_df['sentiment'] = tweets_df['full_text'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)  #-1 to 1"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"iuq1M2UtkqMi","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116791147,"user_tz":-330,"elapsed":1878,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["india_based_user_df = tweets_df.loc[tweets_df['is_user_india_based'].apply(lambda x:bool(x))]\n","outside_india_based_user_df = tweets_df.loc[~tweets_df['is_user_india_based'].apply(lambda x:bool(x))]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"MHWPxBX_lDc4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"status":"ok","timestamp":1599116792774,"user_tz":-330,"elapsed":3481,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"26ba4fd1-8a9d-4696-a0eb-9d6daa269bf9"},"source":["display(tweets_df.shape)\n","display(india_based_user_df.shape)\n","display(outside_india_based_user_df.shape)"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["(791511, 4)"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["(104310, 4)"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["(687201, 4)"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"RxKfLSpQoAb4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1599116792777,"user_tz":-330,"elapsed":3454,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"ebb13a9f-43ee-4ce7-e598-9504bc2b4318"},"source":["tweets_df.head()"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>full_text</th>\n","      <th>is_user_india_based</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1223489863552421888</td>\n","      <td>normala sister talkin of viability gap funding...</td>\n","      <td>0</td>\n","      <td>0.25</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1223729344432001024</td>\n","      <td>air indias 747 carrying evacuees from wuhan ha...</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1223490838589632512</td>\n","      <td>indian already airlifted this is india isme sa...</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1223491079628115968</td>\n","      <td>one takeaway from the coronavirus dont punish ...</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1223491311371575296</td>\n","      <td>an air india flight carrying 324 indian nation...</td>\n","      <td>0</td>\n","      <td>0.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              tweet_id  ... sentiment\n","0  1223489863552421888  ...      0.25\n","1  1223729344432001024  ...      0.00\n","2  1223490838589632512  ...      0.00\n","3  1223491079628115968  ...      0.00\n","4  1223491311371575296  ...      0.00\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"PJuEmXsHi-4S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1599116793831,"user_tz":-330,"elapsed":2087,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"0b720431-0abb-449c-f63e-ca7c923cd9d0"},"source":["##extract the abstract to pandas \n","india_based_documents = india_based_user_df.iloc[:, [1,3]]\n","india_based_documents=india_based_documents.reset_index()\n","india_based_documents.drop(\"index\", inplace = True, axis = 1)\n","\n","##create pandas data frame with all abstracts, use as input corpus\n","india_based_documents[\"index\"] = india_based_documents.index.values\n","india_based_documents.head(3)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>full_text</th>\n","      <th>sentiment</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>relieved to see ai 1349 return safely to with ...</td>\n","      <td>0.500000</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>everytime you abuse for its deficient services...</td>\n","      <td>-0.200000</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>air india special flight to evacuate indian ci...</td>\n","      <td>0.357143</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           full_text  sentiment  index\n","0  relieved to see ai 1349 return safely to with ...   0.500000      0\n","1  everytime you abuse for its deficient services...  -0.200000      1\n","2  air india special flight to evacuate indian ci...   0.357143      2"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"JQLUZiWBfGYx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1599116794417,"user_tz":-330,"elapsed":2273,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"c097158e-c283-4c9d-bf4d-180ebe322a1b"},"source":["##extract the abstract to pandas \n","outside_india_based_documents = outside_india_based_user_df.iloc[:, [1,3]]\n","outside_india_based_documents=outside_india_based_documents.reset_index()\n","outside_india_based_documents.drop(\"index\", inplace = True, axis = 1)\n","\n","##create pandas data frame with all abstracts, use as input corpus\n","outside_india_based_documents[\"index\"] = outside_india_based_documents.index.values\n","outside_india_based_documents.head(3)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>full_text</th>\n","      <th>sentiment</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>normala sister talkin of viability gap funding...</td>\n","      <td>0.25</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>air indias 747 carrying evacuees from wuhan ha...</td>\n","      <td>0.00</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>indian already airlifted this is india isme sa...</td>\n","      <td>0.00</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           full_text  sentiment  index\n","0  normala sister talkin of viability gap funding...       0.25      0\n","1  air indias 747 carrying evacuees from wuhan ha...       0.00      1\n","2  indian already airlifted this is india isme sa...       0.00      2"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"H26Scgzti-4a","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"3v4Wsl-7i-4a","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116800117,"user_tz":-330,"elapsed":5784,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["from collections import Counter\n","from sklearn.metrics import silhouette_score\n","import umap\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from gensim.models.coherencemodel import CoherenceModel\n","import numpy as np\n","import os\n","\n","\n","def get_topic_words(token_lists, labels, k=None):\n","    \"\"\"\n","    get top words within each topic from clustering results\n","    \"\"\"\n","    if k is None:\n","        k = len(np.unique(labels))\n","    topics = ['' for _ in range(k)]\n","    for i, c in enumerate(token_lists):\n","        topics[labels[i]] += (' ' + ' '.join(c))\n","    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n","    # get sorted word counts\n","    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n","    # get topics\n","    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n","\n","    return topics\n","\n","def get_coherence(model, token_lists, measure='c_v'):\n","    \"\"\"\n","    Get model coherence from gensim.models.coherencemodel\n","    :param model: Topic_Model object\n","    :param token_lists: token lists of docs\n","    :param topics: topics as top words\n","    :param measure: coherence metrics\n","    :return: coherence score\n","    \"\"\"\n","    if model.method == 'LDA':\n","        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n","                            coherence=measure)\n","    else:\n","        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n","        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n","                            coherence=measure)\n","    return cm.get_coherence()\n","\n","def get_silhouette(model):\n","    \"\"\"\n","    Get silhouette score from model\n","    :param model: Topic_Model object\n","    :return: silhouette score\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    lbs = model.cluster_model.labels_\n","    vec = model.vec[model.method]\n","    return silhouette_score(vec, lbs)\n","\n","def plot_proj(embedding, lbs):\n","    \"\"\"\n","    Plot UMAP embeddings\n","    :param embedding: UMAP (or other) embeddings\n","    :param lbs: labels\n","    \"\"\"\n","    n = len(embedding)\n","    counter = Counter(lbs)\n","    for i in range(len(np.unique(lbs))):\n","        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n","                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n","    plt.legend(loc = 'best')\n","    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n","\n","\n","def visualize(model):\n","    \"\"\"\n","    Visualize the result for the topic model by 2D embedding (UMAP)\n","    :param model: Topic_Model object\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","\n","    if model.prefix == None:\n","        model.prefix = model.method\n","\n","    reducer = umap.UMAP()\n","    print('Calculating UMAP projection ...')\n","    vec_umap = reducer.fit_transform(model.vec[model.method])\n","    print('Calculating UMAP projection. Done!')\n","    plot_proj(vec_umap, model.cluster_model.labels_)\n","    dr = '/content/drive/My Drive/tweets_analysis/working/contextual_topic_identification/docs/images/{}/{}/{}'.format(model.prefix, model.method, model.id)\n","    if not os.path.exists(dr):\n","        os.makedirs(dr)\n","    plt.savefig('/content/drive/My Drive/tweets_analysis/working/{}/2D_vis'.format(model.prefix))\n","\n","def get_wordcloud(model, token_lists, topic):\n","    \"\"\"\n","    Get word cloud of each topic from fitted model\n","    :param model: Topic_Model object\n","    :param sentences: preprocessed sentences from docs\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","\n","    if model.prefix == None:\n","        model.prefix = model.method\n","\n","    print('Getting wordcloud for topic {} ...'.format(topic))\n","    lbs = model.cluster_model.labels_\n","    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n","\n","    wordcloud = WordCloud(width=800, height=560,\n","                          background_color='white', collocations=False,\n","                          min_font_size=10).generate(tokens)\n","\n","    # plot the WordCloud image\n","    plt.figure(figsize=(8, 5.6), facecolor=None)\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.tight_layout(pad=0)\n","    dr = '/content/drive/My Drive/tweets_analysis/working/{}/{}/{}'.format(model.prefix, model.method, model.id)\n","    if not os.path.exists(dr):\n","        os.makedirs(dr)\n","    plt.savefig('/content/drive/My Drive/tweets_analysis/working' + '/' + model.prefix + '/Topic' + str(topic) + '_wordcloud')\n","    print('Getting wordcloud for topic {}. Done!'.format(topic))"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clCbqjw9i-4c","colab_type":"text"},"source":["## Preprocessing "]},{"cell_type":"code","metadata":{"id":"5qaTWeZzi-4d","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116810446,"user_tz":-330,"elapsed":10320,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["from stop_words import get_stop_words\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from language_detector import detect_language\n","\n","import pkg_resources\n","from symspellpy import SymSpell, Verbosity\n","\n","sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n","dictionary_path = pkg_resources.resource_filename(\n","    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","if sym_spell.word_count:\n","    pass\n","else:\n","    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","\n","\n","###################################\n","#### sentence level preprocess ####\n","###################################\n","\n","# lowercase + base filter\n","# some basic normalization\n","def f_base(s):\n","    \"\"\"\n","    :param s: string to be processed\n","    :return: processed string: see comments in the source code for more info\n","    \"\"\"\n","    # normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n","    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n","    # normalization 2: lower case\n","    s = s.lower()\n","    # normalization 3: \"&gt\", \"&lt\"\n","    s = re.sub(r'&gt|&lt', ' ', s)\n","    # normalization 4: letter repetition (if more than 2)\n","    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n","    # normalization 5: non-word repetition (if more than 1)\n","    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n","    # normalization 6: string * as delimiter\n","    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n","    # normalization 7: stuff in parenthesis, assumed to be less informal\n","    s = re.sub(r'\\(.*?\\)', '. ', s)\n","    # normalization 8: xxx[?!]. -- > xxx.\n","    s = re.sub(r'\\W+?\\.', '.', s)\n","    # normalization 9: [.?!] --> [.?!] xxx\n","    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n","    # normalization 10: ' ing ', noise text\n","    s = re.sub(r' ing ', ' ', s)\n","    # normalization 11: noise text\n","    s = re.sub(r'product received for free[.| ]', ' ', s)\n","    # normalization 12: phrase repetition\n","    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)\n","\n","    return s.strip()\n","\n","\n","# language detection\n","def f_lan(s):\n","    \"\"\"\n","    :param s: string to be processed\n","    :return: boolean (s is English)\n","    \"\"\"\n","\n","    # some reviews are actually english but biased toward french\n","    return detect_language(s) in {'English'}\n","    # return detect_language(s) in {'English', 'French','Spanish','Chinese'}\n","\n","\n","###############################\n","#### word level preprocess ####\n","###############################\n","\n","# filtering out punctuations and numbers\n","def f_punct(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with punct and number filter out\n","    \"\"\"\n","    return [word for word in w_list if word.isalpha()]\n","\n","\n","# selecting nouns\n","def f_noun(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with only nouns selected\n","    \"\"\"\n","    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n","\n","\n","# typo correction\n","def f_typo(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n","    \"\"\"\n","    w_list_fixed = []\n","    for word in w_list:\n","        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n","        if suggestions:\n","            w_list_fixed.append(suggestions[0].term)\n","        else:\n","            pass\n","            # do word segmentation, deprecated for inefficiency\n","            # w_seg = sym_spell.word_segmentation(phrase=word)\n","            # w_list_fixed.extend(w_seg.corrected_string.split())\n","    return w_list_fixed\n","\n","\n","# stemming if doing word-wise\n","p_stemmer = PorterStemmer()\n","\n","\n","def f_stem(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with stemming\n","    \"\"\"\n","    return [p_stemmer.stem(word) for word in w_list]\n","\n","\n","# filtering out stop words\n","# create English stop words list\n","\n","stop_words = (list(\n","    set(get_stop_words('en'))\n","    # |set(get_stop_words('es'))\n","    # |set(get_stop_words('de'))\n","    # |set(get_stop_words('it'))\n","    # |set(get_stop_words('ca'))\n","    #|set(get_stop_words('cy'))\n","    # |set(get_stop_words('pt'))\n","    #|set(get_stop_words('tl'))\n","    # |set(get_stop_words('pl'))\n","    #|set(get_stop_words('et'))\n","    # |set(get_stop_words('da'))\n","    # |set(get_stop_words('ru'))\n","    #|set(get_stop_words('so'))\n","    # |set(get_stop_words('sv'))\n","    # |set(get_stop_words('sk'))\n","    #|set(get_stop_words('cs'))\n","    # |set(get_stop_words('nl'))\n","    #|set(get_stop_words('sl'))\n","    #|set(get_stop_words('no'))\n","    #|set(get_stop_words('zh-cn'))\n","))\n","\n","\n","\n","\n","\n","def f_stopw(w_list):\n","    \"\"\"\n","    filtering out stop words\n","    \"\"\"\n","    return [word for word in w_list if word not in stop_words]\n","\n","\n","def preprocess_sent(rw):\n","    \"\"\"\n","    Get sentence level preprocessed data from raw review texts\n","    :param rw: review to be processed\n","    :return: sentence level pre-processed review\n","    \"\"\"\n","    s = f_base(rw)\n","    if not f_lan(s):\n","        return None\n","    return s\n","\n","\n","def preprocess_word(s):\n","    \"\"\"\n","    Get word level preprocessed data from preprocessed sentences\n","    including: remove punctuation, select noun, fix typo, stem, stop_words\n","    :param s: sentence to be processed\n","    :return: word level pre-processed review\n","    \"\"\"\n","    if not s:\n","        return None\n","    w_list = word_tokenize(s)\n","    w_list = f_punct(w_list)\n","    w_list = f_noun(w_list)\n","    w_list = f_typo(w_list)\n","    w_list = f_stem(w_list)\n","    w_list = f_stopw(w_list)\n","\n","    return w_list"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"glF4eaIYi-4f","colab_type":"text"},"source":["## Autoencoder"]},{"cell_type":"code","metadata":{"id":"fR-_nDgwi-4g","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116813417,"user_tz":-330,"elapsed":13276,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["import keras\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from sklearn.model_selection import train_test_split\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class Autoencoder:\n","    \"\"\"\n","    Autoencoder for learning latent space representation\n","    architecture simplified for only one hidden layer\n","    \"\"\"\n","\n","    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n","        self.latent_dim = latent_dim\n","        self.activation = activation\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.autoencoder = None\n","        self.encoder = None\n","        self.decoder = None\n","        self.his = None\n","\n","    def _compile(self, input_dim):\n","        \"\"\"\n","        compile the computational graph\n","        \"\"\"\n","        input_vec = Input(shape=(input_dim,))\n","        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n","        decoded = Dense(input_dim, activation=self.activation)(encoded)\n","        self.autoencoder = Model(input_vec, decoded)\n","        self.encoder = Model(input_vec, encoded)\n","        encoded_input = Input(shape=(self.latent_dim,))\n","        decoder_layer = self.autoencoder.layers[-1]\n","        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n","        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n","\n","    def fit(self, X):\n","        if not self.autoencoder:\n","            self._compile(X.shape[1])\n","        X_train, X_test = train_test_split(X)\n","        self.his = self.autoencoder.fit(X_train, X_train,\n","                                        epochs=self.epochs,\n","                                        batch_size=self.batch_size,\n","                                        shuffle=True,\n","                                        validation_data=(X_test, X_test), verbose=0)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qIfcUGnki-4i","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"LOBKuDVhi-4i","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116817637,"user_tz":-330,"elapsed":15227,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from gensim import corpora\n","import gensim\n","import numpy as np\n","#from Autoencoder import *\n","#from preprocess import *\n","from datetime import datetime\n","from sentence_transformers import SentenceTransformer\n","from transformers import AutoTokenizer, AutoModel\n","import torch\n","\n","def preprocess(docs, sentiment_scores, samp_size=None):\n","    \"\"\"\n","    Preprocess the data\n","    \"\"\"\n","    if not samp_size:\n","        samp_size = 100\n","\n","    print('Preprocessing raw texts ...')\n","    n_docs = len(docs)\n","    sentences = []  # sentence level preprocessed\n","    sentiments = []\n","    token_lists = []  # word level preprocessed\n","    idx_in = []  # index of sample selected\n","    #     samp = list(range(100))\n","    samp = np.random.choice(n_docs, samp_size)\n","    print('Total samples : {}'.format(len(samp)))\n","    for i, idx in enumerate(samp):\n","        sentence = preprocess_sent(docs[idx])\n","        sentiment = sentiment_scores[idx]\n","        token_list = preprocess_word(sentence)\n","        if token_list:\n","            idx_in.append(idx)\n","            sentences.append(sentence)\n","            sentiments.append(sentiment)\n","            token_lists.append(token_list)\n","        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n","    print('Preprocessing raw texts. Done!')\n","    return sentences, sentiments, token_lists, idx_in\n","\n","\n","# define model object\n","class Topic_Model:\n","    def __init__(self, k=10, method='TFIDF', prefix=None, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n","        \"\"\"\n","        :param k: number of topics\n","        :param method: method chosen for the topic model\n","        \"\"\"\n","        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n","            raise Exception('Invalid method!')\n","        self.k = k\n","        self.dictionary = None\n","        self.corpus = None\n","        # self.stopwords = None\n","        self.cluster_model = None\n","        self.ldamodel = None\n","        self.vec = {}\n","        self.gamma = 15  # parameter for reletive importance of lda\n","        self.method = method\n","        self.AE = None\n","        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n","        self.prefix = prefix\n","        self.latent_dim=latent_dim\n","        self.activation=activation\n","        self.epochs=epochs\n","        self.batch_size=batch_size\n","\n","    #Scale the 'sentiment' vector\n","    #Sentiment varies from -1 to +1\n","    def sentiment(self, x):\n","        if x < 0.04:\n","            return 0\n","        elif x > 0.04:\n","            return 1\n","        else:\n","            return 0.5\n","\n","    #Mean Pooling - Take attention mask into account for correct averaging\n","    def mean_pooling(self, model_output, attention_mask):\n","        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        return sum_embeddings / sum_mask\n","    \n","    #Max Pooling - Take the max value over time for every dimension\n","    def max_pooling(self, model_output, attention_mask):\n","        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n","        max_over_time = torch.max(token_embeddings, 1)[0]\n","        return max_over_time\n","\n","    def encode_bert_vec(self, sentences):\n","        print('Getting vector representations for BERT ...')\n","        #  # model = SentenceTransformer('bert-base-nli-max-tokens')\n","        #   model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n","        #   vec = np.array(model.encode(sentences, show_progress_bar=True))\n","        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/roberta-large-nli-stsb-mean-tokens\")\n","        model = AutoModel.from_pretrained(\"sentence-transformers/roberta-large-nli-stsb-mean-tokens\")\n","\n","        #Tokenize sentences\n","        encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","\n","        #Compute token embeddings\n","        with torch.no_grad():\n","            model_output = model(**encoded_input)\n","\n","        #Perform pooling. In this case, max pooling\n","        sentence_embeddings = self.mean_pooling(model_output, encoded_input['attention_mask'])\n","\n","        print('Getting vector representations for BERT. Done!')\n","\n","        vec = np.array(sentence_embeddings)\n","\n","        return vec\n","\n","    def vectorize(self, sentences, token_lists, sentiments, method=None):\n","        \"\"\"\n","        Get vector representations from selected methods\n","        \"\"\"\n","        # Default method\n","        if method is None:\n","            method = self.method\n","\n","        # turn tokenized documents into a id <-> term dictionary\n","        self.dictionary = corpora.Dictionary(token_lists)\n","        # convert tokenized documents into a document-term matrix\n","        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","\n","        if method == 'TFIDF':\n","            print('Getting vector representations for TF-IDF ...')\n","            tfidf = TfidfVectorizer()\n","            vec = tfidf.fit_transform(sentences)\n","            print('Getting vector representations for TF-IDF. Done!')\n","            return vec\n","\n","        elif method == 'LDA':\n","            print('Getting vector representations for LDA ...')\n","            if not self.ldamodel:\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n","                                                                passes=20)\n","\n","            def get_vec_lda(model, corpus, k):\n","                \"\"\"\n","                Get the LDA vector representation (probabilistic topic assignments for all documents)\n","                :return: vec_lda with dimension: (n_doc * n_topic)\n","                \"\"\"\n","                n_doc = len(corpus)\n","                vec_lda = np.zeros((n_doc, k))\n","                for i in range(n_doc):\n","                    # get the distribution for the i-th document in corpus\n","                    for topic, prob in model.get_document_topics(corpus[i]):\n","                        vec_lda[i, topic] = prob\n","\n","                return vec_lda\n","\n","            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n","            print('Getting vector representations for LDA. Done!')\n","            return vec\n","\n","        elif method == 'BERT':\n","\n","            # print('Getting vector representations for BERT ...')\n","            # # model = SentenceTransformer('bert-base-nli-max-tokens')\n","            # model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n","            # vec = np.array(model.encode(sentences, show_progress_bar=True))\n","            # print('Getting vector representations for BERT. Done!')\n","            vec = self.encode_bert_vec(sentences)\n","            return vec\n","\n","             \n","        elif method == 'LDA_BERT':\n","        #else:\n","            vec_lda = self.vectorize(sentences, token_lists, sentiments, method='LDA')\n","            vec_bert = self.vectorize(sentences, token_lists, sentiments, method='BERT')\n","\n","            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n","\n","            vec_ldabert_df = pd.DataFrame(vec_ldabert)\n","\n","            vec_ldabert_df = pd.concat([vec_ldabert_df, pd.Series(sentiments).apply(lambda x: self.sentiment(x))], axis=1)\n","\n","            vec_ldabert = vec_ldabert_df.to_numpy()\n","\n","            self.vec['LDA_BERT_FULL'] = vec_ldabert\n","            if not self.AE:\n","                self.AE = Autoencoder(self.latent_dim, self.activation, self.epochs, self.batch_size)\n","                print('Fitting Autoencoder ...')\n","                self.AE.fit(vec_ldabert)\n","                print('Fitting Autoencoder Done!')\n","            vec = self.AE.encoder.predict(vec_ldabert)\n","            return vec\n","\n","    def fit(self, sentences, token_lists, sentiments, method=None, m_clustering=None):\n","        \"\"\"\n","        Fit the topic model for selected method given the preprocessed data\n","        :docs: list of documents, each doc is preprocessed as tokens\n","        :return:\n","        \"\"\"\n","        # Default method\n","        if method is None:\n","            method = self.method\n","        # Default clustering method\n","        if m_clustering is None:\n","            m_clustering = KMeans\n","\n","        # turn tokenized documents into a id <-> term dictionary\n","        if not self.dictionary:\n","            self.dictionary = corpora.Dictionary(token_lists)\n","            # convert tokenized documents into a document-term matrix\n","            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","\n","        ####################################################\n","        #### Getting ldamodel or vector representations ####\n","        ####################################################\n","\n","        if method == 'LDA':\n","            if not self.ldamodel:\n","                print('Fitting LDA ...')\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n","                                                                passes=20)\n","                print('Fitting LDA Done!')\n","        else:\n","            print('Clustering embeddings ...')\n","            self.cluster_model = m_clustering(self.k)\n","            self.vec[method] = self.vectorize(sentences, token_lists, sentiments, method)\n","            self.cluster_model.fit(self.vec[method])\n","            print('Clustering embeddings. Done!')\n","\n","    def predict(self, sentences, token_lists, out_of_sample=None):\n","        \"\"\"\n","        Predict topics for new_documents\n","        \"\"\"\n","        # Default as False\n","        out_of_sample = out_of_sample is not None\n","\n","        if out_of_sample:\n","            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","            if self.method != 'LDA':\n","                vec = self.vectorize(sentences, token_lists)\n","                print(vec)\n","        else:\n","            corpus = self.corpus\n","            vec = self.vec.get(self.method, None)\n","\n","        if self.method == \"LDA\":\n","            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n","                                                     key=lambda x: x[1], reverse=True)[0][0],\n","                                    corpus)))\n","        else:\n","            lbs = self.cluster_model.predict(vec)\n","        return lbs"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VV8Lquaii-4l","colab_type":"text"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"eJ0r8oRsi-4m","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116817638,"user_tz":-330,"elapsed":12774,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["#from model import *\n","#from utils import *\n","\n","import pandas as pd\n","import pickle\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore', category=Warning)\n","\n","import argparse\n","\n","#def model(): #:if __name__ == '__main__':\n","\n","def main(documents, samp_size, prefix=None, ntopic=10, method='TFIDF', \n","         latent_dim=32, activation='relu', epochs=200, batch_size=128, \n","         show_visualize=True):\n","    # ntopic = 10\n","    \n","    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n","\n","    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n","    #parser.add_argument('--ntopic', default=10,)\n","    #parser.add_argument('--method', default='TFIDF')\n","    #parser.add_argument('--samp_size', default=20500)\n","    \n","    #args = parser.parse_args()\n","\n","    data = documents #pd.read_csv('/kaggle/working/train.csv')\n","    data = data.fillna('')  # only the comments has NaN's\n","    rws = data.full_text\n","    sentiment_scores = data.sentiment\n","    sentences, sentiments, token_lists, idx_in = preprocess(rws, sentiment_scores, samp_size=samp_size)\n","    print('Total preprocessed sentences : {}'.format(len(sentences)))\n","    print('Total associated sentiments : {}'.format(len(sentiments)))\n","    # Define the topic model object\n","    #tm = Topic_Model(k = 10), method = TFIDF)\n","    tm = Topic_Model(k = ntopic, method = method, prefix=prefix, \n","                     latent_dim=latent_dim, activation=activation, \n","                     epochs=epochs, batch_size=batch_size)\n","    # Fit the topic model by chosen method\n","    tm.fit(sentences, token_lists, sentiments)\n","    # Evaluate using metrics\n","    # with open(\"/content/drive/My Drive/tweets_analysis/working/{}.file\".format(tm.id), \"wb\") as f:\n","    #     pickle.dump(tm, f, pickle.HIGHEST_PROTOCOL)\n","    # file_pi = open(\"/content/drive/My Drive/tweets_analysis/working/{}.file\".format(tm.id), \"w\") \n","    # pickle.dump(tm, file_pi)\n","\n","    coherence_sc = get_coherence(tm, token_lists, 'c_v')\n","    silhouette_sc = get_silhouette(tm)\n","\n","    print('Coherence:', coherence_sc)\n","    print('Silhouette Score:', silhouette_sc)\n","    # visualize and save img\n","    if show_visualize:\n","        visualize(tm)\n","\n","        for i in range(tm.k):\n","            get_wordcloud(tm, token_lists, i)\n","\n","    return (tm, coherence_sc, silhouette_sc)\n","        \n"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"I5UikvKzoTP-","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116820112,"user_tz":-330,"elapsed":2462,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["# compute coherence value at various values of alpha and num_topics\n","def compute_coherence_values(documents, samp_size, prefix, num_topics_range, latent_dims_range, \n","                             method = 'LDA_BERT', activation='relu', epochs=200, \n","                             batch_size=128, show_visualize=True):\n","    \n","    coherence_values = []\n","    silhouette_values = []\n","    model_list = []\n","    for latent_dim in latent_dims_range:\n","        for num_topics in num_topics_range:\n","            prefix = '{}_{}_{}'.format(prefix, num_topics, latent_dim)\n","            lda_model, coherence_sc, silhouette_sc = main(documents, samp_size, prefix=prefix, ntopic=num_topics, method=method, \n","                                                          latent_dim=latent_dim, activation=activation, epochs=epochs, batch_size=batch_size, show_visualize=show_visualize)\n","            model_list.append(lda_model)\n","            coherence_values.append((latent_dim, num_topics, coherence_sc))\n","            silhouette_values.append((latent_dim, num_topics, silhouette_sc))\n","            print()\n","        \n","\n","    return model_list, coherence_values, silhouette_values"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIpPa7cqi-4o","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":174},"outputId":"8263e640-10cb-46a4-d563-46274667e2cf"},"source":["prefix='india_based' \n","activation='relu'\n","epochs=100\n","batch_size=128\n","method = \"LDA_BERT\"\n","samp_size=10000 # len(documents.index)\n","show_visualize=False\n","\n","num_topics_range = [2, 6, 10, 15, 20]\n","latent_dims_range = [16, 32, 64, 128]\n","\n","# num_topics_range = [2]\n","# latent_dims_range = [128]\n","\n","model_list, coherence_values, silhouette_values = compute_coherence_values(india_based_documents, samp_size, prefix, num_topics_range, \n","                         latent_dims_range, method, activation, epochs, batch_size,\n","                         show_visualize)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Preprocessing raw texts ...\n","Total samples : 10000\n","Preprocessing raw texts. Done!\n","Total preprocessed sentences : 9170\n","Total associated sentiments : 9170\n","Clustering embeddings ...\n","Getting vector representations for LDA ...\n","Getting vector representations for LDA. Done!\n","Getting vector representations for BERT ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bKCKXD_rRevQ","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1599069302592,"user_tz":-330,"elapsed":207404,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["coherence_df = pd.DataFrame(coherence_values, columns=['alpha', 'num_topics', 'coherence_value'])\n","coherence_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"miwDLBVQRdGU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-T5HlDngd5r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207,"referenced_widgets":["4bb6b376f0fb44b5938886267fb74728","700ab3aa4be94dcb89dff8cedbd0e279","efb38403471d412ea5d2c78a8b2df998","8ec65b24b0414cc28b3a942ef32fe7b6","898a166bdb2c465ba9a93dc0b749bf23","1d6a04bd3795490198d0367f4f93e6ef","f767e3e5cf40469a8a5cf74517d4b157","28339c149a5447718beaefb58d0f6282"]},"outputId":"5fbfca47-f6f2-4133-aa31-435e2b0d38d5"},"source":["outside_india_based_tm = main(outside_india_based_documents)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Preprocessing raw texts ...\n","Preprocessing raw texts. Done!\n","Clustering embeddings ...\n","Getting vector representations for LDA ...\n","Getting vector representations for LDA. Done!\n","Getting vector representations for BERT ...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bb6b376f0fb44b5938886267fb74728","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Batches', max=78293.0, style=ProgressStyle(description_wiâ€¦"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Getting vector representations for BERT. Done!\n","Fitting Autoencoder ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ry0Bfc8puqOp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599116931391,"user_tz":-330,"elapsed":1777,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}}},"source":["tm = model_list[0]"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"E5LRftXazIcE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599117049137,"user_tz":-330,"elapsed":1841,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"cecb05cc-e0be-49c9-8dd2-3bf6ac72b2b2"},"source":["%cd /content/drive/My Drive/tweets_analysis\n","\n","!mkdir working"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/tweets_analysis\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"A3HRIIz-C5Fu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":288},"executionInfo":{"status":"error","timestamp":1599117505558,"user_tz":-330,"elapsed":1826,"user":{"displayName":"Apoorv Bhardwaj","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjYLrPUrGZItRK6R23BmZVTZY_tokQNxgr9E3E5=s64","userId":"11838558921289780057"}},"outputId":"3415637e-e52e-4234-ceb3-9216d265615b"},"source":["%xmode Verbose\n","file_pi = open(\"/content/drive/My Drive/tweets_analysis/working/{}.file\".format(tm.id), \"w\") \n","pickle.dump(tm, file_pi)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Exception reporting mode: Verbose\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-f381e4df10a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'xmode Verbose'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/tweets_analysis/working/{}.file\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mpickle.dump\u001b[0m \u001b[0;34m= <built-in function dump>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mtm\u001b[0m \u001b[0;34m= <__main__.Topic_Model object at 0x7f2fbb25b710>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mfile_pi\u001b[0m \u001b[0;34m= <_io.TextIOWrapper name='/content/drive/My Drive/tweets_analysis/working/LDA_BERT_2020_09_03_07_06_59.file' mode='w' encoding='UTF-8'>\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"]}]},{"cell_type":"code","metadata":{"id":"uo-QAcftmDod","colab_type":"code","colab":{}},"source":["file_pi = open(\"/content/drive/My Drive/tweets_analysis/working/{}.file\".format(tm.id), \"w\") \n","pickle.dump(outside_india_based_tm, file_pi)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kwodVomHRZ-z","colab_type":"text"},"source":["# Step 6: Calculate abstraction and expression for each narrative \n","Note that each cluster represents a narrative"]},{"cell_type":"code","metadata":{"id":"9zccfzXURZ-0","colab_type":"code","colab":{}},"source":["tweets_to_consider = 'fully_cleaned_tweet'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xPmqRdAWRZ-2","colab_type":"code","colab":{},"outputId":"bb01cd4f-7b05-4d2b-c2ee-2c6342523771"},"source":["final_clusters = np.unique(dfUnique['cl_num'])\n","print(final_clusters)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[ 1  2  3  5  7  8  9 10]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lhM5_r_4RZ-5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2ZiN6mZRZ-7","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jUfYJIieRZ-_","colab_type":"code","colab":{}},"source":["#Store all tweets corresponding to each cluster in a file\n","for i in final_clusters:\n","    with open('./tweets_Cluster_'+str(i)+'.txt','w') as out:\n","        y = ''\n","        for x in dfUnique[tweets_to_consider][dfUnique.cl_num == i]:    \n","            y = y + x + '. '\n","        out.write(y)\n","        out.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z49BD8-WRZ_B","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7b_26HFRZ_D","colab_type":"code","colab":{},"outputId":"08a6abd7-5a9b-4154-dca5-659f55153b13"},"source":["#A combination of (Noun, adjective, cardinal number, foreign word and Verb) are being extracted now\n","#Extract chunks matching pattern. Patterns are:\n","#1) Noun phrase (2 or more nouns occurring together. Ex United states of America, Abdul Kalam etc)\n","#2) Number followed by Noun (Ex: 28 Terrorists, 45th President)\n","#3) Adjective followed by Noun (Ex: Economic impact, beautiful inauguration)\n","#4) Foreign word (Ex: Jallikattu, Narendra modi, Pappu)\n","#5) Noun followed by Verb (Ex: Terrorists arrested)\n","#And a combination of all 5\n","        \n","import re\n","import nltk\n","\n","phrases = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n","\n","\n","A = '(CD|JJ)/\\w+\\s'  #cd or jj\n","B = '(NN|NNS|NNP|NNPS)/\\w+\\s'  #nouns\n","C = '(VB|VBD|VBG|VBN|VBP|VBZ)/\\w+\\s' #verbs\n","D = 'FW/\\w+\\s'  #foreign word\n","patterns = ['('+A+B+')+', '('+D+B+')+','('+D+')+', '('+B+')+', '('+D+A+B+')+', \n","           '('+B+C+')+', '('+D+B+C+')+', '('+B+A+B+')+', '('+B+B+C+')+'] \n","\n","\n","def extract_phrases(tag1, tag2, sentences):\n","    extract_phrase = []\n","    for sentence in sentences:\n","        phrase = []\n","        next_word = 0\n","        for word, pos in nltk.pos_tag(nltk.word_tokenize(sentence)):\n","            if next_word == 1:\n","                next_word = 0\n","                if pos == tag2:\n","                    extract_phrase = np.append(extract_phrase,phrase + ' ' + word) \n","            \n","            if pos == tag1:\n","                next_word = 1\n","                phrase = word\n","    return extract_phrase\n","\n","for i in cluster_name:\n","    File = open('./tweets_Cluster_'+str(i)+'.txt', 'r') #open file\n","    lines = File.read() #read all lines\n","    sentences = nltk.sent_tokenize(lines) #tokenize sentences\n","\n","    for sentence in sentences: \n","        f = nltk.pos_tag(nltk.word_tokenize(sentence))\n","        tag_seq = []\n","        for word, pos in f:\n","            tag_seq.append(pos+'/'+ word)\n","        X = \" \".join(tag_seq)\n","\n","        phrase = []\n","        for j in range(len(patterns)):\n","            if re.search(patterns[j], X):\n","                phrase.append(' '.join([word.split('/')[1] for word in re.search(patterns[j], X).group(0).split()]))\n","    \n","        k = pd.DataFrame({'extracted_phrases': np.unique(phrase), 'cluster_num': int(i)})\n","    \n","        phrases = pd.concat([phrases,k], ignore_index = True)\n","\n","print(phrases)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["      cluster_num              extracted_phrases\n","0             1.0   centerofright demonetization\n","1             1.0                 demonetization\n","2             1.0                      scale has\n","3             1.0                 demonetization\n","4             1.0                 demonetization\n","5             1.0                 demonetization\n","6             1.0                 demonetization\n","7             1.0                 demonetization\n","8             1.0                   huge support\n","9             1.0             nation 8086 people\n","10            1.0                        support\n","11            1.0                 demonetization\n","12            1.0                 demonetization\n","13            1.0                 eliminate jobs\n","14            1.0                   huge support\n","15            1.0             nation 8086 people\n","16            1.0                        support\n","17            1.0                    money daily\n","18            1.0                   huge support\n","19            1.0             nation 8086 people\n","20            1.0                        support\n","21            1.0                 demonetization\n","22            1.0    demonetization huge success\n","23            1.0                   huge success\n","24            1.0                         akhara\n","25            1.0               political akhara\n","26            1.0                 property daily\n","27            1.0        decision demonetization\n","28            1.0         irresponsible decision\n","29            1.0                 demonetization\n","...           ...                            ...\n","6603         10.0                        jairajp\n","6604         10.0            terrorists ensuring\n","6605         10.0         opposition parties are\n","6606         10.0                    parties are\n","6607         10.0                         people\n","6608         10.0                 everyone seems\n","6609         10.0       kanimozhi everyone seems\n","6610         10.0  philosophy kanimozhi everyone\n","6611         10.0   centerofright demonetization\n","6612         10.0                 demonetization\n","6613         10.0   centerofright demonetization\n","6614         10.0           demonetization malls\n","6615         10.0                    markets are\n","6616         10.0                        atheist\n","6617         10.0                atheist krishna\n","6618         10.0                atheist krishna\n","6619         10.0                   gandhi heard\n","6620         10.0   centerofright demonetization\n","6621         10.0             demonetization ppl\n","6622         10.0         demonetization ppl are\n","6623         10.0                        ppl are\n","6624         10.0                 day day chanda\n","6625         10.0                           news\n","6626         10.0                     queues are\n","6627         10.0                        economy\n","6628         10.0                 indian economy\n","6629         10.0                        2 weeks\n","6630         10.0           governor hasn spoken\n","6631         10.0                    hasn spoken\n","6632         10.0  replacement rbi governor hasn\n","\n","[6633 rows x 2 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zSwHm8K0RZ_F","colab_type":"text"},"source":["### Keeping the largest phrase"]},{"cell_type":"code","metadata":{"id":"lXMeXK7FRZ_G","colab_type":"code","colab":{}},"source":["#For each phrase identified replace all the substrings by the largest phrase \n","#Ex: lakh looted,40 lakh looted and Rs 40 lakh looted, replace all by single largest phrase - Rs 40 lakh looted \n","#i.e. instead of 3 different phrases, there will be only one large phrase\n","\n","phrases_final = pd.DataFrame({'extracted_phrases': [], 'cluster_num': []})\n","for i in cluster_name:\n","    phrases_for_each_cluster = []\n","    cluster_phrases = phrases['extracted_phrases'][phrases.cluster_num == i]\n","    cluster_phrases = np.unique(np.array(cluster_phrases))\n","    for j in range(len(cluster_phrases)):\n","        \n","        phrase = cluster_phrases[j]\n","        updated_cluster_phrases = np.delete((cluster_phrases), j)\n","        if any(phrase in phr for phr in updated_cluster_phrases): \n","            'y'\n","        else: \n","            #considering phrases of length greater than 1 only\n","            if (len(phrase.split(' '))) > 1:\n","                phrases_for_each_cluster.append(phrase)\n","    k = pd.DataFrame({'extracted_phrases': phrases_for_each_cluster, 'cluster_num': int(i) })\n","    \n","    phrases_final = pd.concat([phrases_final,k], ignore_index = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAkOscxcRZ_I","colab_type":"code","colab":{},"outputId":"7c53a4e3-3c9a-4123-ec8a-841875e7c09f"},"source":["phrases_final"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster_num</th>\n","      <th>extracted_phrases</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1st phase</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>93 support</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>abcyoutubech are</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>ananthkumar bjp</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>arshadwarsi thanks</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.0</td>\n","      <td>bjp modi has</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.0</td>\n","      <td>byelection bjp has</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1.0</td>\n","      <td>bypolls bjp has</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.0</td>\n","      <td>centerofright demonetization</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.0</td>\n","      <td>common people</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1.0</td>\n","      <td>congratulations india has</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1.0</td>\n","      <td>current policy</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1.0</td>\n","      <td>decision demonetization</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1.0</td>\n","      <td>demonetization hollow move</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1.0</td>\n","      <td>demonetization huge step</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1.0</td>\n","      <td>demonetization huge success</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1.0</td>\n","      <td>demonetization lifeinsurance</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1.0</td>\n","      <td>demonetization move has</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1.0</td>\n","      <td>demonetization programme needs</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1.0</td>\n","      <td>demonetization real state</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1.0</td>\n","      <td>demonetization survey</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1.0</td>\n","      <td>dhakad vsl thanks</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1.0</td>\n","      <td>efforts undermine demonetization</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1.0</td>\n","      <td>eliminate jobs</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1.0</td>\n","      <td>establishment curtail indias</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1.0</td>\n","      <td>excellent ananthkumar</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>1.0</td>\n","      <td>exceptional response</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1.0</td>\n","      <td>feel vindicated</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>1.0</td>\n","      <td>gitanjali huge support</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1.0</td>\n","      <td>good citizens</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2951</th>\n","      <td>10.0</td>\n","      <td>imfindia growth</td>\n","    </tr>\n","    <tr>\n","      <th>2952</th>\n","      <td>10.0</td>\n","      <td>indian economy</td>\n","    </tr>\n","    <tr>\n","      <th>2953</th>\n","      <td>10.0</td>\n","      <td>indirect benefit</td>\n","    </tr>\n","    <tr>\n","      <th>2954</th>\n","      <td>10.0</td>\n","      <td>janlokpaal goes</td>\n","    </tr>\n","    <tr>\n","      <th>2955</th>\n","      <td>10.0</td>\n","      <td>kanimozhi everyone seems</td>\n","    </tr>\n","    <tr>\n","      <th>2956</th>\n","      <td>10.0</td>\n","      <td>kumar vishvas drkumarvishwas</td>\n","    </tr>\n","    <tr>\n","      <th>2957</th>\n","      <td>10.0</td>\n","      <td>lionelmedia google</td>\n","    </tr>\n","    <tr>\n","      <th>2958</th>\n","      <td>10.0</td>\n","      <td>markets are</td>\n","    </tr>\n","    <tr>\n","      <th>2959</th>\n","      <td>10.0</td>\n","      <td>mayankjain100 demonetization communal</td>\n","    </tr>\n","    <tr>\n","      <th>2960</th>\n","      <td>10.0</td>\n","      <td>mlas 50 demonetization</td>\n","    </tr>\n","    <tr>\n","      <th>2961</th>\n","      <td>10.0</td>\n","      <td>name pakistan army</td>\n","    </tr>\n","    <tr>\n","      <th>2962</th>\n","      <td>10.0</td>\n","      <td>narendramodi claimed</td>\n","    </tr>\n","    <tr>\n","      <th>2963</th>\n","      <td>10.0</td>\n","      <td>nation debating</td>\n","    </tr>\n","    <tr>\n","      <th>2964</th>\n","      <td>10.0</td>\n","      <td>nationalists join</td>\n","    </tr>\n","    <tr>\n","      <th>2965</th>\n","      <td>10.0</td>\n","      <td>negativity peoples mind</td>\n","    </tr>\n","    <tr>\n","      <th>2966</th>\n","      <td>10.0</td>\n","      <td>nosylviaplath aap</td>\n","    </tr>\n","    <tr>\n","      <th>2967</th>\n","      <td>10.0</td>\n","      <td>offline cash getting</td>\n","    </tr>\n","    <tr>\n","      <th>2968</th>\n","      <td>10.0</td>\n","      <td>opposition parties are</td>\n","    </tr>\n","    <tr>\n","      <th>2969</th>\n","      <td>10.0</td>\n","      <td>oscar goes</td>\n","    </tr>\n","    <tr>\n","      <th>2970</th>\n","      <td>10.0</td>\n","      <td>payments initiative</td>\n","    </tr>\n","    <tr>\n","      <th>2971</th>\n","      <td>10.0</td>\n","      <td>perks benefits</td>\n","    </tr>\n","    <tr>\n","      <th>2972</th>\n","      <td>10.0</td>\n","      <td>philosophy kanimozhi everyone</td>\n","    </tr>\n","    <tr>\n","      <th>2973</th>\n","      <td>10.0</td>\n","      <td>queues are</td>\n","    </tr>\n","    <tr>\n","      <th>2974</th>\n","      <td>10.0</td>\n","      <td>raheelk riding</td>\n","    </tr>\n","    <tr>\n","      <th>2975</th>\n","      <td>10.0</td>\n","      <td>replacement rbi governor hasn</td>\n","    </tr>\n","    <tr>\n","      <th>2976</th>\n","      <td>10.0</td>\n","      <td>samajwadi partys jaya bachchan</td>\n","    </tr>\n","    <tr>\n","      <th>2977</th>\n","      <td>10.0</td>\n","      <td>shirishkunder minor inconvenience</td>\n","    </tr>\n","    <tr>\n","      <th>2978</th>\n","      <td>10.0</td>\n","      <td>terrorists ensuring</td>\n","    </tr>\n","    <tr>\n","      <th>2979</th>\n","      <td>10.0</td>\n","      <td>wave growth hacking</td>\n","    </tr>\n","    <tr>\n","      <th>2980</th>\n","      <td>10.0</td>\n","      <td>youtube are</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2981 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["      cluster_num                      extracted_phrases\n","0             1.0                              1st phase\n","1             1.0                             93 support\n","2             1.0                       abcyoutubech are\n","3             1.0                        ananthkumar bjp\n","4             1.0                     arshadwarsi thanks\n","5             1.0                           bjp modi has\n","6             1.0                     byelection bjp has\n","7             1.0                        bypolls bjp has\n","8             1.0           centerofright demonetization\n","9             1.0                          common people\n","10            1.0              congratulations india has\n","11            1.0                         current policy\n","12            1.0                decision demonetization\n","13            1.0             demonetization hollow move\n","14            1.0               demonetization huge step\n","15            1.0            demonetization huge success\n","16            1.0           demonetization lifeinsurance\n","17            1.0                demonetization move has\n","18            1.0         demonetization programme needs\n","19            1.0              demonetization real state\n","20            1.0                  demonetization survey\n","21            1.0                      dhakad vsl thanks\n","22            1.0       efforts undermine demonetization\n","23            1.0                         eliminate jobs\n","24            1.0           establishment curtail indias\n","25            1.0                  excellent ananthkumar\n","26            1.0                   exceptional response\n","27            1.0                        feel vindicated\n","28            1.0                 gitanjali huge support\n","29            1.0                          good citizens\n","...           ...                                    ...\n","2951         10.0                        imfindia growth\n","2952         10.0                         indian economy\n","2953         10.0                       indirect benefit\n","2954         10.0                        janlokpaal goes\n","2955         10.0               kanimozhi everyone seems\n","2956         10.0           kumar vishvas drkumarvishwas\n","2957         10.0                     lionelmedia google\n","2958         10.0                            markets are\n","2959         10.0  mayankjain100 demonetization communal\n","2960         10.0                 mlas 50 demonetization\n","2961         10.0                     name pakistan army\n","2962         10.0                   narendramodi claimed\n","2963         10.0                        nation debating\n","2964         10.0                      nationalists join\n","2965         10.0                negativity peoples mind\n","2966         10.0                      nosylviaplath aap\n","2967         10.0                   offline cash getting\n","2968         10.0                 opposition parties are\n","2969         10.0                             oscar goes\n","2970         10.0                    payments initiative\n","2971         10.0                         perks benefits\n","2972         10.0          philosophy kanimozhi everyone\n","2973         10.0                             queues are\n","2974         10.0                         raheelk riding\n","2975         10.0          replacement rbi governor hasn\n","2976         10.0         samajwadi partys jaya bachchan\n","2977         10.0      shirishkunder minor inconvenience\n","2978         10.0                    terrorists ensuring\n","2979         10.0                    wave growth hacking\n","2980         10.0                            youtube are\n","\n","[2981 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"wYCymdO3RZ_K","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7mMYxf1RZ_M","colab_type":"code","colab":{},"outputId":"6082ab2a-2cd7-4f10-f74d-bde0668cf930"},"source":["phrases_final.columns"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['cluster_num', 'extracted_phrases'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"UEfombNHRZ_O","colab_type":"text"},"source":["### For each phrase in each cluster, calculate term frequency "]},{"cell_type":"code","metadata":{"id":"I1IthqcURZ_O","colab_type":"code","colab":{},"outputId":"6ee4f4e4-adb0-432d-eded-d872900500f3"},"source":["#Term-frequency : For each cluster, calculate the number of times a given phrase occur in the tweets of that cluster\n","\n","phrases_final['term_freq'] = len(phrases_final)*[0]\n","\n","for i in cluster_name:\n","    for phrase in phrases_final['extracted_phrases'][phrases_final.cluster_num == i]:\n","        tweets = dfUnique[tweets_to_consider][dfUnique.cl_num == i]\n","        for tweet in tweets:\n","            if phrase in tweet:\n","                phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['term_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"9CN-gVXNRZ_R","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqApU1JLRZ_T","colab_type":"text"},"source":["### For each phrase in each cluster, calculate document frequency"]},{"cell_type":"code","metadata":{"id":"Jp1_X7-kRZ_U","colab_type":"code","colab":{},"outputId":"32e86508-8760-4c72-84d1-6a8f3ec8560c"},"source":["#Document-frequency\n","phrases_final['doc_freq'] = len(phrases_final)*[0]\n","\n","\n","# for each phrase, compute the number of clusters that Sphrase occurs in\n","for phrase in phrases_final['extracted_phrases']:\n","    for i in cluster_name:\n","        all_tweets = ''\n","        for tweet in dfUnique[tweets_to_consider][dfUnique.cl_num == i]:\n","            all_tweets = all_tweets + tweet + '. ' \n","        if phrase in all_tweets:\n","            phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] = phrases_final['doc_freq'][(phrases_final.extracted_phrases == phrase) & (phrases_final.cluster_num == i)] + 1\n","        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  if sys.path[0] == '':\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"cpLg7APtRZ_Y","colab_type":"code","colab":{}},"source":["import math\n","phrases_final['doc_freq'] = phrases_final['doc_freq'].apply(lambda x: math.log10(n_best_clusters/(x)) )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qI8QimpwRZ_a","colab_type":"text"},"source":["### For each phrase in each cluster, calculate tf-idf"]},{"cell_type":"code","metadata":{"id":"_Lgu_h5aRZ_a","colab_type":"code","colab":{}},"source":["phrases_final['tf-idf'] = phrases_final['term_freq']*phrases_final['doc_freq']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lrYsWb0ERZ_c","colab_type":"code","colab":{},"outputId":"def4804d-9b1c-4f5c-8f48-3e57ef57eefd"},"source":["phrases_final"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cluster_num</th>\n","      <th>extracted_phrases</th>\n","      <th>term_freq</th>\n","      <th>doc_freq</th>\n","      <th>tf-idf</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1.0</td>\n","      <td>1st phase</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>93 support</td>\n","      <td>1</td>\n","      <td>0.740363</td>\n","      <td>0.740363</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>abcyoutubech are</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>ananthkumar bjp</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1.0</td>\n","      <td>arshadwarsi thanks</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1.0</td>\n","      <td>bjp modi has</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1.0</td>\n","      <td>byelection bjp has</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1.0</td>\n","      <td>bypolls bjp has</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1.0</td>\n","      <td>centerofright demonetization</td>\n","      <td>1</td>\n","      <td>0.564271</td>\n","      <td>0.564271</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1.0</td>\n","      <td>common people</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1.0</td>\n","      <td>congratulations india has</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1.0</td>\n","      <td>current policy</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1.0</td>\n","      <td>decision demonetization</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1.0</td>\n","      <td>demonetization hollow move</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1.0</td>\n","      <td>demonetization huge step</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1.0</td>\n","      <td>demonetization huge success</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1.0</td>\n","      <td>demonetization lifeinsurance</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1.0</td>\n","      <td>demonetization move has</td>\n","      <td>1</td>\n","      <td>0.740363</td>\n","      <td>0.740363</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1.0</td>\n","      <td>demonetization programme needs</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1.0</td>\n","      <td>demonetization real state</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1.0</td>\n","      <td>demonetization survey</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1.0</td>\n","      <td>dhakad vsl thanks</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1.0</td>\n","      <td>efforts undermine demonetization</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1.0</td>\n","      <td>eliminate jobs</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1.0</td>\n","      <td>establishment curtail indias</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1.0</td>\n","      <td>excellent ananthkumar</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>1.0</td>\n","      <td>exceptional response</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1.0</td>\n","      <td>feel vindicated</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>1.0</td>\n","      <td>gitanjali huge support</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1.0</td>\n","      <td>good citizens</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2951</th>\n","      <td>10.0</td>\n","      <td>imfindia growth</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2952</th>\n","      <td>10.0</td>\n","      <td>indian economy</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2953</th>\n","      <td>10.0</td>\n","      <td>indirect benefit</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2954</th>\n","      <td>10.0</td>\n","      <td>janlokpaal goes</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2955</th>\n","      <td>10.0</td>\n","      <td>kanimozhi everyone seems</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>2956</th>\n","      <td>10.0</td>\n","      <td>kumar vishvas drkumarvishwas</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2957</th>\n","      <td>10.0</td>\n","      <td>lionelmedia google</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2958</th>\n","      <td>10.0</td>\n","      <td>markets are</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>2959</th>\n","      <td>10.0</td>\n","      <td>mayankjain100 demonetization communal</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2960</th>\n","      <td>10.0</td>\n","      <td>mlas 50 demonetization</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2961</th>\n","      <td>10.0</td>\n","      <td>name pakistan army</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2962</th>\n","      <td>10.0</td>\n","      <td>narendramodi claimed</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>2963</th>\n","      <td>10.0</td>\n","      <td>nation debating</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>2964</th>\n","      <td>10.0</td>\n","      <td>nationalists join</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>2965</th>\n","      <td>10.0</td>\n","      <td>negativity peoples mind</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2966</th>\n","      <td>10.0</td>\n","      <td>nosylviaplath aap</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2967</th>\n","      <td>10.0</td>\n","      <td>offline cash getting</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2968</th>\n","      <td>10.0</td>\n","      <td>opposition parties are</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2969</th>\n","      <td>10.0</td>\n","      <td>oscar goes</td>\n","      <td>4</td>\n","      <td>1.041393</td>\n","      <td>4.165571</td>\n","    </tr>\n","    <tr>\n","      <th>2970</th>\n","      <td>10.0</td>\n","      <td>payments initiative</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2971</th>\n","      <td>10.0</td>\n","      <td>perks benefits</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2972</th>\n","      <td>10.0</td>\n","      <td>philosophy kanimozhi everyone</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2973</th>\n","      <td>10.0</td>\n","      <td>queues are</td>\n","      <td>1</td>\n","      <td>0.740363</td>\n","      <td>0.740363</td>\n","    </tr>\n","    <tr>\n","      <th>2974</th>\n","      <td>10.0</td>\n","      <td>raheelk riding</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2975</th>\n","      <td>10.0</td>\n","      <td>replacement rbi governor hasn</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","    <tr>\n","      <th>2976</th>\n","      <td>10.0</td>\n","      <td>samajwadi partys jaya bachchan</td>\n","      <td>5</td>\n","      <td>1.041393</td>\n","      <td>5.206963</td>\n","    </tr>\n","    <tr>\n","      <th>2977</th>\n","      <td>10.0</td>\n","      <td>shirishkunder minor inconvenience</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2978</th>\n","      <td>10.0</td>\n","      <td>terrorists ensuring</td>\n","      <td>4</td>\n","      <td>1.041393</td>\n","      <td>4.165571</td>\n","    </tr>\n","    <tr>\n","      <th>2979</th>\n","      <td>10.0</td>\n","      <td>wave growth hacking</td>\n","      <td>1</td>\n","      <td>1.041393</td>\n","      <td>1.041393</td>\n","    </tr>\n","    <tr>\n","      <th>2980</th>\n","      <td>10.0</td>\n","      <td>youtube are</td>\n","      <td>2</td>\n","      <td>1.041393</td>\n","      <td>2.082785</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2981 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["      cluster_num                      extracted_phrases  term_freq  doc_freq  \\\n","0             1.0                              1st phase          1  1.041393   \n","1             1.0                             93 support          1  0.740363   \n","2             1.0                       abcyoutubech are          1  1.041393   \n","3             1.0                        ananthkumar bjp          1  1.041393   \n","4             1.0                     arshadwarsi thanks          1  1.041393   \n","5             1.0                           bjp modi has          1  1.041393   \n","6             1.0                     byelection bjp has          2  1.041393   \n","7             1.0                        bypolls bjp has          1  1.041393   \n","8             1.0           centerofright demonetization          1  0.564271   \n","9             1.0                          common people          1  1.041393   \n","10            1.0              congratulations india has          1  1.041393   \n","11            1.0                         current policy          1  1.041393   \n","12            1.0                decision demonetization          2  1.041393   \n","13            1.0             demonetization hollow move          1  1.041393   \n","14            1.0               demonetization huge step          1  1.041393   \n","15            1.0            demonetization huge success          1  1.041393   \n","16            1.0           demonetization lifeinsurance          2  1.041393   \n","17            1.0                demonetization move has          1  0.740363   \n","18            1.0         demonetization programme needs          1  1.041393   \n","19            1.0              demonetization real state          1  1.041393   \n","20            1.0                  demonetization survey          1  1.041393   \n","21            1.0                      dhakad vsl thanks          1  1.041393   \n","22            1.0       efforts undermine demonetization          1  1.041393   \n","23            1.0                         eliminate jobs          1  1.041393   \n","24            1.0           establishment curtail indias          1  1.041393   \n","25            1.0                  excellent ananthkumar          1  1.041393   \n","26            1.0                   exceptional response          1  1.041393   \n","27            1.0                        feel vindicated          2  1.041393   \n","28            1.0                 gitanjali huge support          1  1.041393   \n","29            1.0                          good citizens          1  1.041393   \n","...           ...                                    ...        ...       ...   \n","2951         10.0                        imfindia growth          1  1.041393   \n","2952         10.0                         indian economy          1  1.041393   \n","2953         10.0                       indirect benefit          1  1.041393   \n","2954         10.0                        janlokpaal goes          1  1.041393   \n","2955         10.0               kanimozhi everyone seems          2  1.041393   \n","2956         10.0           kumar vishvas drkumarvishwas          1  1.041393   \n","2957         10.0                     lionelmedia google          1  1.041393   \n","2958         10.0                            markets are          2  1.041393   \n","2959         10.0  mayankjain100 demonetization communal          1  1.041393   \n","2960         10.0                 mlas 50 demonetization          1  1.041393   \n","2961         10.0                     name pakistan army          1  1.041393   \n","2962         10.0                   narendramodi claimed          2  1.041393   \n","2963         10.0                        nation debating          2  1.041393   \n","2964         10.0                      nationalists join          2  1.041393   \n","2965         10.0                negativity peoples mind          1  1.041393   \n","2966         10.0                      nosylviaplath aap          1  1.041393   \n","2967         10.0                   offline cash getting          1  1.041393   \n","2968         10.0                 opposition parties are          1  1.041393   \n","2969         10.0                             oscar goes          4  1.041393   \n","2970         10.0                    payments initiative          1  1.041393   \n","2971         10.0                         perks benefits          1  1.041393   \n","2972         10.0          philosophy kanimozhi everyone          1  1.041393   \n","2973         10.0                             queues are          1  0.740363   \n","2974         10.0                         raheelk riding          1  1.041393   \n","2975         10.0          replacement rbi governor hasn          2  1.041393   \n","2976         10.0         samajwadi partys jaya bachchan          5  1.041393   \n","2977         10.0      shirishkunder minor inconvenience          1  1.041393   \n","2978         10.0                    terrorists ensuring          4  1.041393   \n","2979         10.0                    wave growth hacking          1  1.041393   \n","2980         10.0                            youtube are          2  1.041393   \n","\n","        tf-idf  \n","0     1.041393  \n","1     0.740363  \n","2     1.041393  \n","3     1.041393  \n","4     1.041393  \n","5     1.041393  \n","6     2.082785  \n","7     1.041393  \n","8     0.564271  \n","9     1.041393  \n","10    1.041393  \n","11    1.041393  \n","12    2.082785  \n","13    1.041393  \n","14    1.041393  \n","15    1.041393  \n","16    2.082785  \n","17    0.740363  \n","18    1.041393  \n","19    1.041393  \n","20    1.041393  \n","21    1.041393  \n","22    1.041393  \n","23    1.041393  \n","24    1.041393  \n","25    1.041393  \n","26    1.041393  \n","27    2.082785  \n","28    1.041393  \n","29    1.041393  \n","...        ...  \n","2951  1.041393  \n","2952  1.041393  \n","2953  1.041393  \n","2954  1.041393  \n","2955  2.082785  \n","2956  1.041393  \n","2957  1.041393  \n","2958  2.082785  \n","2959  1.041393  \n","2960  1.041393  \n","2961  1.041393  \n","2962  2.082785  \n","2963  2.082785  \n","2964  2.082785  \n","2965  1.041393  \n","2966  1.041393  \n","2967  1.041393  \n","2968  1.041393  \n","2969  4.165571  \n","2970  1.041393  \n","2971  1.041393  \n","2972  1.041393  \n","2973  0.740363  \n","2974  1.041393  \n","2975  2.082785  \n","2976  5.206963  \n","2977  1.041393  \n","2978  4.165571  \n","2979  1.041393  \n","2980  2.082785  \n","\n","[2981 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"n9HMaBWRRZ_e","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqRXunztRZ_f","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyXECl0yRZ_h","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpUjq9dpRZ_k","colab_type":"text"},"source":["### For each cluster find top few phrases and respective sentiment\n"," "]},{"cell_type":"code","metadata":{"id":"2acj69riRZ_l","colab_type":"code","colab":{},"outputId":"e4fa199f-abe4-4563-fb0b-d5beb8d0a4fb"},"source":["phrases_final['diff_tf-idf'] = len(phrases_final)*[0]\n","\n","narrative = pd.DataFrame({'cl_num': [], 'abstraction': []})\n","for i in cluster_name: \n","    # arrange in descending order of tf-idf score\n","    phrases_final = phrases_final.sort_values(['cluster_num','tf-idf'], ascending=[1,0])\n","    \n","    #Break this distribution at a point where the difference between any consecutive phrases is maximum\n","    #difference between consecutive values of tf-idf \n","    phrases_final['diff_tf-idf'][phrases_final.cluster_num == i] = abs(phrases_final['tf-idf'][phrases_final.cluster_num == i] - phrases_final['tf-idf'][phrases_final.cluster_num == i].shift(1))\n","\n","    #The last value for each cluster will be 'NaN'. Replacing it with '0'. \n","    phrases_final = phrases_final.fillna(0)\n","    \n","    phrases_final = phrases_final.reset_index(drop = True) #to avoid old index being added as a new column\n","    if len(phrases_final[phrases_final.cluster_num == i]) != 0:\n","        \n","        #index corresponding to the highest difference\n"," \n","        ind = (phrases_final['diff_tf-idf'][phrases_final.cluster_num == i]).idxmax()\n","        \n","        abstract = phrases_final['extracted_phrases'][:ind+1][phrases_final.cluster_num == i]\n","    \n","    \n","        #store the abstraction corresponding to each cluster\n","        k = pd.DataFrame({'cl_num': int(i), 'abstraction': abstract})\n","        narrative = pd.concat([narrative,k], ignore_index = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  # Remove the CWD from sys.path while we load stuff.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xCMNtQfARZ_n","colab_type":"code","colab":{},"outputId":"a4c11961-39d6-4ed2-e555-fd47f246634a"},"source":["dfUnique"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet</th>\n","      <th>cleaned_tweet</th>\n","      <th>fully_cleaned_tweet</th>\n","      <th>sentiment</th>\n","      <th>tokenized_tweet</th>\n","      <th>cl_num</th>\n","      <th>freq</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2500</th>\n","      <td>RT @centerofright: #DeMonetization - Finally -...</td>\n","      <td>centerofright   demonetization  finally   coun...</td>\n","      <td>centerofright demonetization finally country s...</td>\n","      <td>1</td>\n","      <td>[centerofright, demonetization, finally, count...</td>\n","      <td>1</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>3136</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3137</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3138</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3139</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3140</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1466</th>\n","      <td>Huge support for #PM #NarendraModii â€™s #demone...</td>\n","      <td>huge support for    narendramodii    demonetiz...</td>\n","      <td>huge support for narendramodii demonetization ...</td>\n","      <td>1</td>\n","      <td>[huge, support, for, narendramodii, demonetiza...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3141</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3142</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3143</th>\n","      <td>The latest The I Eliminate Jobs Daily! https:/...</td>\n","      <td>the latest the  eliminate jobs daily</td>\n","      <td>the latest the eliminate jobs daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, eliminate, jobs, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1469</th>\n","      <td>Huge support for PM narendramodi â€™s #demonetiz...</td>\n","      <td>huge support for  narendramodi    demonetizati...</td>\n","      <td>huge support for narendramodi demonetization m...</td>\n","      <td>1</td>\n","      <td>[huge, support, for, narendramodi, demonetizat...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3144</th>\n","      <td>The latest The Money Daily! https://t.co/HdJy5...</td>\n","      <td>the latest the money daily</td>\n","      <td>the latest the money daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, money, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1468</th>\n","      <td>Huge support for PM @narendramodi â€™s #demoneti...</td>\n","      <td>huge support for   narendramodi    demonetizat...</td>\n","      <td>huge support for narendramodi demonetization m...</td>\n","      <td>1</td>\n","      <td>[huge, support, for, narendramodi, demonetizat...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1263</th>\n","      <td>Demonetization is a huge success&lt;ed&gt;&lt;U+00A0&gt;&lt;U...</td>\n","      <td>demonetization   huge success</td>\n","      <td>demonetization huge success</td>\n","      <td>1</td>\n","      <td>[demonetization, huge, success]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3145</th>\n","      <td>The latest The Political Akhara Daily! https:/...</td>\n","      <td>the latest the political akhara daily</td>\n","      <td>the latest the political akhara daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, political, akhara, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3146</th>\n","      <td>The latest The Property Daily! https://t.co/QS...</td>\n","      <td>the latest the property daily</td>\n","      <td>the latest the property daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, property, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>917</th>\n","      <td>@narendramodi should step down for the most ir...</td>\n","      <td>should step down for the most irresponsible d...</td>\n","      <td>should step down for the most irresponsible de...</td>\n","      <td>1</td>\n","      <td>[should, step, down, for, the, most, irrespons...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2672</th>\n","      <td>RT @mituamin: The latest The Demonetization Da...</td>\n","      <td>mituamin  the latest the demonetization daily</td>\n","      <td>mituamin the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[mituamin, the, latest, the, demonetization, d...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2195</th>\n","      <td>RT @ParvinSharma_: The #Demonetization is a hu...</td>\n","      <td>parvinsharma   the  demonetization   huge step...</td>\n","      <td>parvinsharma the demonetization huge step agai...</td>\n","      <td>1</td>\n","      <td>[parvinsharma, the, demonetization, huge, step...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3135</th>\n","      <td>The latest The Demonetization Daily! https://t...</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>the latest the demonetization daily</td>\n","      <td>1</td>\n","      <td>[the, latest, the, demonetization, daily]</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2328</th>\n","      <td>RT @TheDarjChron: #Demonetization #NorthEast R...</td>\n","      <td>thedarjchron   demonetization  northeast react...</td>\n","      <td>thedarjchron demonetization northeast reaction...</td>\n","      <td>1</td>\n","      <td>[thedarjchron, demonetization, northeast, reac...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2210</th>\n","      <td>RT @RNTata2000: The governmentâ€™s bold implemen...</td>\n","      <td>rntata2000  the government  bold implementatio...</td>\n","      <td>rntata2000 the government bold implementation ...</td>\n","      <td>1</td>\n","      <td>[the, government, bold, implementation, the, d...</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3034</th>\n","      <td>Take the #demonetization survey and pls put an...</td>\n","      <td>take the  demonetization survey and pls put  h...</td>\n","      <td>take the demonetization survey and pls put hon...</td>\n","      <td>1</td>\n","      <td>[take, the, demonetization, survey, and, pls, ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1893</th>\n","      <td>RT @ABCYoutubeCh: Are the common people happy ...</td>\n","      <td>abcyoutubech  are the common people happy with...</td>\n","      <td>abcyoutubech are the common people happy with ...</td>\n","      <td>1</td>\n","      <td>[abcyoutubech, are, the, common, people, happy...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2153</th>\n","      <td>RT @ModiBharosa: Huge support for PM @narendra...</td>\n","      <td>modibharosa  huge support for   narendramodi  ...</td>\n","      <td>modibharosa huge support for narendramodi demo...</td>\n","      <td>1</td>\n","      <td>[modibharosa, huge, support, for, narendramodi...</td>\n","      <td>1</td>\n","      <td>246</td>\n","    </tr>\n","    <tr>\n","      <th>1209</th>\n","      <td>Congratulations India has gone back to barter ...</td>\n","      <td>congratulations india has gone back  barter sy...</td>\n","      <td>congratulations india has gone back barter sys...</td>\n","      <td>0</td>\n","      <td>[congratulations, india, has, gone, back, bart...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1467</th>\n","      <td>Huge support for PM @narendramodi â€™s #demoneti...</td>\n","      <td>huge support for   narendramodi    demonetizat...</td>\n","      <td>huge support for narendramodi demonetization m...</td>\n","      <td>1</td>\n","      <td>[huge, support, for, narendramodi, demonetizat...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1867</th>\n","      <td>Post the #Indian currency #Demonetization move...</td>\n","      <td>post the  indian currency  demonetization move...</td>\n","      <td>post the indian currency demonetization move b...</td>\n","      <td>0</td>\n","      <td>[post, the, indian, currency, demonetization, ...</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2212</th>\n","      <td>RT @Ra_THORe: #IndiaFightsCorruption The hones...</td>\n","      <td>thore   indiafightscorruption the honest feel...</td>\n","      <td>thore indiafightscorruption the honest feel vi...</td>\n","      <td>1</td>\n","      <td>[thore, indiafightscorruption, the, honest, fe...</td>\n","      <td>1</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>2838</th>\n","      <td>RT @sundeepgummadi: Post demonetization Real s...</td>\n","      <td>sundeepgummadi  post demonetization real state...</td>\n","      <td>sundeepgummadi post demonetization real state ...</td>\n","      <td>1</td>\n","      <td>[sundeepgummadi, post, demonetization, real, s...</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1269</th>\n","      <td>Demonetization successfully established.\\r\\nCr...</td>\n","      <td>demonetization successfully established credit...</td>\n","      <td>demonetization successfully established credit...</td>\n","      <td>1</td>\n","      <td>[demonetization, successfully, established, cr...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2970</th>\n","      <td>Samajwadi Party's Jaya Bachchan and AAP join T...</td>\n","      <td>samajwadi partys jaya bachchan and aap join tm...</td>\n","      <td>samajwadi partys jaya bachchan and aap join tm...</td>\n","      <td>0</td>\n","      <td>[samajwadi, partys, jaya, bachchan, and, aap, ...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1313</th>\n","      <td>Effect of #demonetization and #GST on Indian #...</td>\n","      <td>effect   demonetization and  gst  indian  clea...</td>\n","      <td>effect demonetization and gst indian cleaningbiz</td>\n","      <td>0</td>\n","      <td>[effect, demonetization, and, gst, indian, cle...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2951</th>\n","      <td>Retweeted Dr Kumar Vishvas (@DrKumarVishwas):\\...</td>\n","      <td>retweeted  kumar vishvas  drkumarvishwas   and...</td>\n","      <td>retweeted kumar vishvas drkumarvishwas and the...</td>\n","      <td>0</td>\n","      <td>[retweeted, kumar, vishvas, drkumarvishwas, an...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2283</th>\n","      <td>RT @SimplyShilpi_89: That is not an ad, they a...</td>\n","      <td>simplyshilpi 89  that  not   they are congratu...</td>\n","      <td>simplyshilpi 89 that not they are congratuting...</td>\n","      <td>0</td>\n","      <td>[simplyshilpi, that, not, they, are, congratut...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>299</th>\n","      <td>#RightTimeToInvest\\r\\nWhile the entire nation ...</td>\n","      <td>righttimetoinvest while the entire nation  de...</td>\n","      <td>righttimetoinvest while the entire nation deba...</td>\n","      <td>0</td>\n","      <td>[righttimetoinvest, while, the, entire, nation...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>#RightTimeToInvest\\r\\nWhile the entire nation ...</td>\n","      <td>righttimetoinvest while the entire nation  de...</td>\n","      <td>righttimetoinvest while the entire nation deba...</td>\n","      <td>0</td>\n","      <td>[righttimetoinvest, while, the, entire, nation...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2934</th>\n","      <td>Reduce the perks &amp;amp; benefits of all MP's an...</td>\n","      <td>reduce the perks    benefits  all mps and mlas...</td>\n","      <td>reduce the perks benefits all mps and mlas 50 ...</td>\n","      <td>0</td>\n","      <td>[reduce, the, perks, benefits, all, mps, and, ...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2027</th>\n","      <td>RT @DrKumarVishwas: And the Oscar goes to \"Mr....</td>\n","      <td>drkumarvishwas  and the oscar goes   demonetiz...</td>\n","      <td>drkumarvishwas and the oscar goes demonetization</td>\n","      <td>0</td>\n","      <td>[drkumarvishwas, and, the, oscar, goes, demone...</td>\n","      <td>10</td>\n","      <td>350</td>\n","    </tr>\n","    <tr>\n","      <th>1616</th>\n","      <td>It Is the extensive use of black money in elec...</td>\n","      <td>the extensive use  black money  elections tha...</td>\n","      <td>the extensive use black money elections that a...</td>\n","      <td>0</td>\n","      <td>[the, extensive, use, black, money, elections,...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>304</th>\n","      <td>#SonuNigam before and after demonetization. ht...</td>\n","      <td>sonunigam before and after demonetization</td>\n","      <td>sonunigam before and after demonetization</td>\n","      <td>0</td>\n","      <td>[sonunigam, before, and, after, demonetization]</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2779</th>\n","      <td>RT @s_manjari: And the crowd is back in the Ma...</td>\n","      <td>manjari  and the crowd  back  the market    d...</td>\n","      <td>manjari and the crowd back the market demoneti...</td>\n","      <td>0</td>\n","      <td>[manjari, and, the, crowd, back, the, market, ...</td>\n","      <td>10</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1597</th>\n","      <td>Indirect benefit of #demonetization?\\r\\nAir in...</td>\n","      <td>indirect benefit   demonetization air  new del...</td>\n","      <td>indirect benefit demonetization air new delhi ...</td>\n","      <td>0</td>\n","      <td>[indirect, benefit, demonetization, air, new, ...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2770</th>\n","      <td>RT @rsprasad: Digital Payments initiative afte...</td>\n","      <td>rsprasad  digital payments initiative after de...</td>\n","      <td>rsprasad digital payments initiative after dem...</td>\n","      <td>0</td>\n","      <td>[rsprasad, digital, payments, initiative, afte...</td>\n","      <td>10</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2667</th>\n","      <td>RT @mayankjain100: #demonetization is Communal...</td>\n","      <td>mayankjain100   demonetization  communal and b...</td>\n","      <td>mayankjain100 demonetization communal and blac...</td>\n","      <td>0</td>\n","      <td>[demonetization, communal, and, black, money, ...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2634</th>\n","      <td>RT @kanimozhi: Everyone seems to hate the rich...</td>\n","      <td>kanimozhi  everyone seems  hate the rich even ...</td>\n","      <td>kanimozhi everyone seems hate the rich even th...</td>\n","      <td>0</td>\n","      <td>[kanimozhi, everyone, seems, hate, the, rich, ...</td>\n","      <td>10</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>2405</th>\n","      <td>RT @airnewsalerts: Samajwadi Party's Jaya Bach...</td>\n","      <td>airnewsalerts  samajwadi partys jaya bachchan ...</td>\n","      <td>airnewsalerts samajwadi partys jaya bachchan a...</td>\n","      <td>0</td>\n","      <td>[airnewsalerts, samajwadi, partys, jaya, bachc...</td>\n","      <td>10</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>2615</th>\n","      <td>RT @jamewils: So they pick pocketing us in the...</td>\n","      <td>jamewils   they pick pocketing   the guise  de...</td>\n","      <td>jamewils they pick pocketing the guise demonet...</td>\n","      <td>0</td>\n","      <td>[jamewils, they, pick, pocketing, the, guise, ...</td>\n","      <td>10</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>2607</th>\n","      <td>RT @jairajp: â€¦and the replacement RBI Governor...</td>\n","      <td>jairajp   and the replacement rbi governor has...</td>\n","      <td>jairajp and the replacement rbi governor hasn ...</td>\n","      <td>0</td>\n","      <td>[jairajp, and, the, replacement, rbi, governor...</td>\n","      <td>10</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>2605</th>\n","      <td>RT @jairajp: There goes another claimed benefi...</td>\n","      <td>jairajp  there goes another claimed benefit   ...</td>\n","      <td>jairajp there goes another claimed benefit dem...</td>\n","      <td>0</td>\n","      <td>[jairajp, there, goes, another, claimed, benef...</td>\n","      <td>10</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1839</th>\n","      <td>People and opposition parties are calling it #...</td>\n","      <td>people and opposition parties are calling   de...</td>\n","      <td>people and opposition parties are calling demo...</td>\n","      <td>0</td>\n","      <td>[people, and, opposition, parties, are, callin...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1849</th>\n","      <td>Philosophy &lt;ed&gt;&lt;U+00A0&gt;&lt;U+00BD&gt;&lt;ed&gt;&lt;U+00B8&gt;&lt;U+...</td>\n","      <td>philosophy     kanimozhi  everyone seems  hate...</td>\n","      <td>philosophy kanimozhi everyone seems hate the r...</td>\n","      <td>0</td>\n","      <td>[philosophy, kanimozhi, everyone, seems, hate,...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2509</th>\n","      <td>RT @centerofright: #DeMonetization - other tha...</td>\n","      <td>centerofright   demonetization  other than the...</td>\n","      <td>centerofright demonetization other than the us...</td>\n","      <td>0</td>\n","      <td>[centerofright, demonetization, other, than, t...</td>\n","      <td>10</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>2507</th>\n","      <td>RT @centerofright: #DeMonetization - malls and...</td>\n","      <td>centerofright   demonetization  malls and mark...</td>\n","      <td>centerofright demonetization malls and markets...</td>\n","      <td>0</td>\n","      <td>[centerofright, demonetization, malls, and, ma...</td>\n","      <td>10</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>1940</th>\n","      <td>RT @Atheist_Krishna: The effect of #Demonetiza...</td>\n","      <td>atheist krishna  the effect   demonetization</td>\n","      <td>atheist krishna the effect demonetization</td>\n","      <td>0</td>\n","      <td>[atheist, krishna, the, effect, demonetization]</td>\n","      <td>10</td>\n","      <td>43</td>\n","    </tr>\n","    <tr>\n","      <th>1939</th>\n","      <td>RT @Atheist_Krishna: BEFORE and AFTER Gandhi j...</td>\n","      <td>atheist krishna  before and after gandhi  hear...</td>\n","      <td>atheist krishna before and after gandhi heard ...</td>\n","      <td>0</td>\n","      <td>[atheist, krishna, before, and, after, gandhi,...</td>\n","      <td>10</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>2503</th>\n","      <td>RT @centerofright: #DeMonetization - Ppl are r...</td>\n","      <td>centerofright   demonetization  ppl are ready ...</td>\n","      <td>centerofright demonetization ppl are ready giv...</td>\n","      <td>0</td>\n","      <td>[centerofright, demonetization, ppl, are, read...</td>\n","      <td>10</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1896</th>\n","      <td>RT @ANI_news: The queues are not getting longe...</td>\n","      <td>ani news  the queues are not getting longer bu...</td>\n","      <td>ani news the queues are not getting longer but...</td>\n","      <td>0</td>\n","      <td>[ani, news, the, queues, are, not, getting, lo...</td>\n","      <td>10</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1988</th>\n","      <td>RT @ChampDev_: Indian economy before and after...</td>\n","      <td>chdev   indian economy before and after demone...</td>\n","      <td>chdev indian economy before and after demoneti...</td>\n","      <td>0</td>\n","      <td>[chdev, indian, economy, before, and, after, d...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3484</th>\n","      <td>â€¦and the replacement RBI Governor hasnâ€™t spoke...</td>\n","      <td>and the replacement rbi governor hasn  spoken...</td>\n","      <td>and the replacement rbi governor hasn spoken e...</td>\n","      <td>0</td>\n","      <td>[and, the, replacement, rbi, governor, hasn, s...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2230 rows Ã— 7 columns</p>\n","</div>"],"text/plain":["                                                  tweet  \\\n","2500  RT @centerofright: #DeMonetization - Finally -...   \n","3136  The latest The Demonetization Daily! https://t...   \n","3137  The latest The Demonetization Daily! https://t...   \n","3138  The latest The Demonetization Daily! https://t...   \n","3139  The latest The Demonetization Daily! https://t...   \n","3140  The latest The Demonetization Daily! https://t...   \n","1466  Huge support for #PM #NarendraModii â€™s #demone...   \n","3141  The latest The Demonetization Daily! https://t...   \n","3142  The latest The Demonetization Daily! https://t...   \n","3143  The latest The I Eliminate Jobs Daily! https:/...   \n","1469  Huge support for PM narendramodi â€™s #demonetiz...   \n","3144  The latest The Money Daily! https://t.co/HdJy5...   \n","1468  Huge support for PM @narendramodi â€™s #demoneti...   \n","1263  Demonetization is a huge success<ed><U+00A0><U...   \n","3145  The latest The Political Akhara Daily! https:/...   \n","3146  The latest The Property Daily! https://t.co/QS...   \n","917   @narendramodi should step down for the most ir...   \n","2672  RT @mituamin: The latest The Demonetization Da...   \n","2195  RT @ParvinSharma_: The #Demonetization is a hu...   \n","3135  The latest The Demonetization Daily! https://t...   \n","2328  RT @TheDarjChron: #Demonetization #NorthEast R...   \n","2210  RT @RNTata2000: The governmentâ€™s bold implemen...   \n","3034  Take the #demonetization survey and pls put an...   \n","1893  RT @ABCYoutubeCh: Are the common people happy ...   \n","2153  RT @ModiBharosa: Huge support for PM @narendra...   \n","1209  Congratulations India has gone back to barter ...   \n","1467  Huge support for PM @narendramodi â€™s #demoneti...   \n","1867  Post the #Indian currency #Demonetization move...   \n","2212  RT @Ra_THORe: #IndiaFightsCorruption The hones...   \n","2838  RT @sundeepgummadi: Post demonetization Real s...   \n","...                                                 ...   \n","1269  Demonetization successfully established.\\r\\nCr...   \n","2970  Samajwadi Party's Jaya Bachchan and AAP join T...   \n","1313  Effect of #demonetization and #GST on Indian #...   \n","2951  Retweeted Dr Kumar Vishvas (@DrKumarVishwas):\\...   \n","2283  RT @SimplyShilpi_89: That is not an ad, they a...   \n","299   #RightTimeToInvest\\r\\nWhile the entire nation ...   \n","300   #RightTimeToInvest\\r\\nWhile the entire nation ...   \n","2934  Reduce the perks &amp; benefits of all MP's an...   \n","2027  RT @DrKumarVishwas: And the Oscar goes to \"Mr....   \n","1616  It Is the extensive use of black money in elec...   \n","304   #SonuNigam before and after demonetization. ht...   \n","2779  RT @s_manjari: And the crowd is back in the Ma...   \n","1597  Indirect benefit of #demonetization?\\r\\nAir in...   \n","2770  RT @rsprasad: Digital Payments initiative afte...   \n","2667  RT @mayankjain100: #demonetization is Communal...   \n","2634  RT @kanimozhi: Everyone seems to hate the rich...   \n","2405  RT @airnewsalerts: Samajwadi Party's Jaya Bach...   \n","2615  RT @jamewils: So they pick pocketing us in the...   \n","2607  RT @jairajp: â€¦and the replacement RBI Governor...   \n","2605  RT @jairajp: There goes another claimed benefi...   \n","1839  People and opposition parties are calling it #...   \n","1849  Philosophy <ed><U+00A0><U+00BD><ed><U+00B8><U+...   \n","2509  RT @centerofright: #DeMonetization - other tha...   \n","2507  RT @centerofright: #DeMonetization - malls and...   \n","1940  RT @Atheist_Krishna: The effect of #Demonetiza...   \n","1939  RT @Atheist_Krishna: BEFORE and AFTER Gandhi j...   \n","2503  RT @centerofright: #DeMonetization - Ppl are r...   \n","1896  RT @ANI_news: The queues are not getting longe...   \n","1988  RT @ChampDev_: Indian economy before and after...   \n","3484  â€¦and the replacement RBI Governor hasnâ€™t spoke...   \n","\n","                                          cleaned_tweet  \\\n","2500  centerofright   demonetization  finally   coun...   \n","3136               the latest the demonetization daily    \n","3137               the latest the demonetization daily    \n","3138               the latest the demonetization daily    \n","3139               the latest the demonetization daily    \n","3140               the latest the demonetization daily    \n","1466  huge support for    narendramodii    demonetiz...   \n","3141               the latest the demonetization daily    \n","3142               the latest the demonetization daily    \n","3143              the latest the  eliminate jobs daily    \n","1469  huge support for  narendramodi    demonetizati...   \n","3144                        the latest the money daily    \n","1468  huge support for   narendramodi    demonetizat...   \n","1263                     demonetization   huge success    \n","3145             the latest the political akhara daily    \n","3146                     the latest the property daily    \n","917    should step down for the most irresponsible d...   \n","2672     mituamin  the latest the demonetization daily    \n","2195  parvinsharma   the  demonetization   huge step...   \n","3135               the latest the demonetization daily    \n","2328  thedarjchron   demonetization  northeast react...   \n","2210  rntata2000  the government  bold implementatio...   \n","3034  take the  demonetization survey and pls put  h...   \n","1893  abcyoutubech  are the common people happy with...   \n","2153  modibharosa  huge support for   narendramodi  ...   \n","1209  congratulations india has gone back  barter sy...   \n","1467  huge support for   narendramodi    demonetizat...   \n","1867  post the  indian currency  demonetization move...   \n","2212   thore   indiafightscorruption the honest feel...   \n","2838  sundeepgummadi  post demonetization real state...   \n","...                                                 ...   \n","1269  demonetization successfully established credit...   \n","2970  samajwadi partys jaya bachchan and aap join tm...   \n","1313  effect   demonetization and  gst  indian  clea...   \n","2951  retweeted  kumar vishvas  drkumarvishwas   and...   \n","2283  simplyshilpi 89  that  not   they are congratu...   \n","299    righttimetoinvest while the entire nation  de...   \n","300    righttimetoinvest while the entire nation  de...   \n","2934  reduce the perks    benefits  all mps and mlas...   \n","2027  drkumarvishwas  and the oscar goes   demonetiz...   \n","1616   the extensive use  black money  elections tha...   \n","304          sonunigam before and after demonetization    \n","2779   manjari  and the crowd  back  the market    d...   \n","1597  indirect benefit   demonetization air  new del...   \n","2770  rsprasad  digital payments initiative after de...   \n","2667  mayankjain100   demonetization  communal and b...   \n","2634  kanimozhi  everyone seems  hate the rich even ...   \n","2405  airnewsalerts  samajwadi partys jaya bachchan ...   \n","2615  jamewils   they pick pocketing   the guise  de...   \n","2607  jairajp   and the replacement rbi governor has...   \n","2605  jairajp  there goes another claimed benefit   ...   \n","1839  people and opposition parties are calling   de...   \n","1849  philosophy     kanimozhi  everyone seems  hate...   \n","2509  centerofright   demonetization  other than the...   \n","2507  centerofright   demonetization  malls and mark...   \n","1940      atheist krishna  the effect   demonetization    \n","1939  atheist krishna  before and after gandhi  hear...   \n","2503  centerofright   demonetization  ppl are ready ...   \n","1896  ani news  the queues are not getting longer bu...   \n","1988  chdev   indian economy before and after demone...   \n","3484   and the replacement rbi governor hasn  spoken...   \n","\n","                                    fully_cleaned_tweet  sentiment  \\\n","2500  centerofright demonetization finally country s...          1   \n","3136                the latest the demonetization daily          1   \n","3137                the latest the demonetization daily          1   \n","3138                the latest the demonetization daily          1   \n","3139                the latest the demonetization daily          1   \n","3140                the latest the demonetization daily          1   \n","1466  huge support for narendramodii demonetization ...          1   \n","3141                the latest the demonetization daily          1   \n","3142                the latest the demonetization daily          1   \n","3143                the latest the eliminate jobs daily          1   \n","1469  huge support for narendramodi demonetization m...          1   \n","3144                         the latest the money daily          1   \n","1468  huge support for narendramodi demonetization m...          1   \n","1263                        demonetization huge success          1   \n","3145              the latest the political akhara daily          1   \n","3146                      the latest the property daily          1   \n","917   should step down for the most irresponsible de...          1   \n","2672       mituamin the latest the demonetization daily          1   \n","2195  parvinsharma the demonetization huge step agai...          1   \n","3135                the latest the demonetization daily          1   \n","2328  thedarjchron demonetization northeast reaction...          1   \n","2210  rntata2000 the government bold implementation ...          1   \n","3034  take the demonetization survey and pls put hon...          1   \n","1893  abcyoutubech are the common people happy with ...          1   \n","2153  modibharosa huge support for narendramodi demo...          1   \n","1209  congratulations india has gone back barter sys...          0   \n","1467  huge support for narendramodi demonetization m...          1   \n","1867  post the indian currency demonetization move b...          0   \n","2212  thore indiafightscorruption the honest feel vi...          1   \n","2838  sundeepgummadi post demonetization real state ...          1   \n","...                                                 ...        ...   \n","1269  demonetization successfully established credit...          1   \n","2970  samajwadi partys jaya bachchan and aap join tm...          0   \n","1313   effect demonetization and gst indian cleaningbiz          0   \n","2951  retweeted kumar vishvas drkumarvishwas and the...          0   \n","2283  simplyshilpi 89 that not they are congratuting...          0   \n","299   righttimetoinvest while the entire nation deba...          0   \n","300   righttimetoinvest while the entire nation deba...          0   \n","2934  reduce the perks benefits all mps and mlas 50 ...          0   \n","2027   drkumarvishwas and the oscar goes demonetization          0   \n","1616  the extensive use black money elections that a...          0   \n","304           sonunigam before and after demonetization          0   \n","2779  manjari and the crowd back the market demoneti...          0   \n","1597  indirect benefit demonetization air new delhi ...          0   \n","2770  rsprasad digital payments initiative after dem...          0   \n","2667  mayankjain100 demonetization communal and blac...          0   \n","2634  kanimozhi everyone seems hate the rich even th...          0   \n","2405  airnewsalerts samajwadi partys jaya bachchan a...          0   \n","2615  jamewils they pick pocketing the guise demonet...          0   \n","2607  jairajp and the replacement rbi governor hasn ...          0   \n","2605  jairajp there goes another claimed benefit dem...          0   \n","1839  people and opposition parties are calling demo...          0   \n","1849  philosophy kanimozhi everyone seems hate the r...          0   \n","2509  centerofright demonetization other than the us...          0   \n","2507  centerofright demonetization malls and markets...          0   \n","1940          atheist krishna the effect demonetization          0   \n","1939  atheist krishna before and after gandhi heard ...          0   \n","2503  centerofright demonetization ppl are ready giv...          0   \n","1896  ani news the queues are not getting longer but...          0   \n","1988  chdev indian economy before and after demoneti...          0   \n","3484  and the replacement rbi governor hasn spoken e...          0   \n","\n","                                        tokenized_tweet  cl_num  freq  \n","2500  [centerofright, demonetization, finally, count...       1    27  \n","3136          [the, latest, the, demonetization, daily]       1     1  \n","3137          [the, latest, the, demonetization, daily]       1     1  \n","3138          [the, latest, the, demonetization, daily]       1     1  \n","3139          [the, latest, the, demonetization, daily]       1     1  \n","3140          [the, latest, the, demonetization, daily]       1     1  \n","1466  [huge, support, for, narendramodii, demonetiza...       1     1  \n","3141          [the, latest, the, demonetization, daily]       1     1  \n","3142          [the, latest, the, demonetization, daily]       1     1  \n","3143         [the, latest, the, eliminate, jobs, daily]       1     1  \n","1469  [huge, support, for, narendramodi, demonetizat...       1     1  \n","3144                   [the, latest, the, money, daily]       1     1  \n","1468  [huge, support, for, narendramodi, demonetizat...       1     1  \n","1263                    [demonetization, huge, success]       1     1  \n","3145       [the, latest, the, political, akhara, daily]       1     1  \n","3146                [the, latest, the, property, daily]       1     1  \n","917   [should, step, down, for, the, most, irrespons...       1     1  \n","2672  [mituamin, the, latest, the, demonetization, d...       1     1  \n","2195  [parvinsharma, the, demonetization, huge, step...       1     2  \n","3135          [the, latest, the, demonetization, daily]       1     1  \n","2328  [thedarjchron, demonetization, northeast, reac...       1     1  \n","2210  [the, government, bold, implementation, the, d...       1     3  \n","3034  [take, the, demonetization, survey, and, pls, ...       1     1  \n","1893  [abcyoutubech, are, the, common, people, happy...       1     2  \n","2153  [modibharosa, huge, support, for, narendramodi...       1   246  \n","1209  [congratulations, india, has, gone, back, bart...       1     1  \n","1467  [huge, support, for, narendramodi, demonetizat...       1     1  \n","1867  [post, the, indian, currency, demonetization, ...       1     1  \n","2212  [thore, indiafightscorruption, the, honest, fe...       1    35  \n","2838  [sundeepgummadi, post, demonetization, real, s...       1     4  \n","...                                                 ...     ...   ...  \n","1269  [demonetization, successfully, established, cr...      10     1  \n","2970  [samajwadi, partys, jaya, bachchan, and, aap, ...      10     1  \n","1313  [effect, demonetization, and, gst, indian, cle...      10     1  \n","2951  [retweeted, kumar, vishvas, drkumarvishwas, an...      10     1  \n","2283  [simplyshilpi, that, not, they, are, congratut...      10     1  \n","299   [righttimetoinvest, while, the, entire, nation...      10     1  \n","300   [righttimetoinvest, while, the, entire, nation...      10     1  \n","2934  [reduce, the, perks, benefits, all, mps, and, ...      10     1  \n","2027  [drkumarvishwas, and, the, oscar, goes, demone...      10   350  \n","1616  [the, extensive, use, black, money, elections,...      10     1  \n","304     [sonunigam, before, and, after, demonetization]      10     1  \n","2779  [manjari, and, the, crowd, back, the, market, ...      10     2  \n","1597  [indirect, benefit, demonetization, air, new, ...      10     1  \n","2770  [rsprasad, digital, payments, initiative, afte...      10     2  \n","2667  [demonetization, communal, and, black, money, ...      10     1  \n","2634  [kanimozhi, everyone, seems, hate, the, rich, ...      10    23  \n","2405  [airnewsalerts, samajwadi, partys, jaya, bachc...      10     8  \n","2615  [jamewils, they, pick, pocketing, the, guise, ...      10    22  \n","2607  [jairajp, and, the, replacement, rbi, governor...      10     4  \n","2605  [jairajp, there, goes, another, claimed, benef...      10     3  \n","1839  [people, and, opposition, parties, are, callin...      10     1  \n","1849  [philosophy, kanimozhi, everyone, seems, hate,...      10     1  \n","2509  [centerofright, demonetization, other, than, t...      10    30  \n","2507  [centerofright, demonetization, malls, and, ma...      10    12  \n","1940    [atheist, krishna, the, effect, demonetization]      10    43  \n","1939  [atheist, krishna, before, and, after, gandhi,...      10    90  \n","2503  [centerofright, demonetization, ppl, are, read...      10     7  \n","1896  [ani, news, the, queues, are, not, getting, lo...      10     6  \n","1988  [chdev, indian, economy, before, and, after, d...      10     1  \n","3484  [and, the, replacement, rbi, governor, hasn, s...      10     1  \n","\n","[2230 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"MO8HSA2fRZ_p","colab_type":"code","colab":{},"outputId":"13cb5d8c-238e-4a7f-fb5b-85eccabb52e4"},"source":["#Assigning polarity based on the sentiment for each tweet 2=negative, 1=positive, 3=neutral\n","dfUnique['polarity'] = np.NaN\n","dfUnique['polarity'][dfUnique.sentiment == 0.5] = \"3\"\n","dfUnique['polarity'][dfUnique.sentiment == 1] = \"1\"\n","dfUnique['polarity'][dfUnique.sentiment == 0] = \"2\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n","  after removing the cwd from sys.path.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"OeoTlRHmRZ_r","colab_type":"text"},"source":["### Assign the sentiment to each extracted phrases\n","count the number of tweets, a phrase has occurred in positive, negative and neutral context. Assign the most occurred sentiment to the phrase"]},{"cell_type":"code","metadata":{"id":"j8h2A-bYRZ_r","colab_type":"code","colab":{},"outputId":"3e6bb279-5815-4cfd-91c7-22c2daa47d58"},"source":["from collections import Counter\n","\n","#find the highest occurring sentiment corresponding to each tweet\n","def find_mode(a):\n","    b = Counter(a).most_common(3)\n","    mode = []; c_max = 0\n","    for a,c in b:\n","        if c>c_max:\n","            c_max = c\n","        if c_max == c:\n","            mode.append(a)  \n","    print(mode)\n","    mode.sort()\n","    print(mode)\n","    \n","    ## if mode is 3&2 i.e. neutral and negative, assign the overall sentiment for that phrase as negative, \n","    ## if mode is 3&1 i.e. neutral and positive, assign the overall sentiment for that phrase as positive,\n","    ## if mode is 2&1 i.e. negative and positive, assign the overall sentiment for that phrase as neutal, \n","    ## if mode is 3&2&1 i.e. negative, positive and neutral, assign the overall sentiment for that phrase as neutral\n","    \n","    if len(mode) == 1:\n","        return mode[0]\n","    \n","    elif (len(mode) == 2) & (mode[1]=='3'):\n","        return mode[0]\n","    else:\n","        return 3\n","    \n","#1=>+ve 2=>-ve 3=>Neutral\n","narrative['expression'] = -1\n","dfUnique = dfUnique.reset_index(drop = True)\n","for i in cluster_name:\n","    tweets = dfUnique[tweets_to_consider][dfUnique.cl_num == i]\n","    abstracts = narrative['abstraction'][narrative.cl_num == i] \n","    for abst in abstracts:\n","        sent = []\n","        for tweet, polarity in zip(dfUnique[tweets_to_consider][dfUnique.cl_num == i], dfUnique['polarity'][dfUnique.cl_num == i]):\n","            if abst in tweet:\n","                sent = np.append(sent, polarity)\n","        \n","        \n","        if len(sent)!=0:\n","            ## if mode is 3&2-2, 3&1-1, 2&1-3, 3&2&1 - 3\n","            senti = find_mode(sent)\n","            if senti == '2':\n","                sent_value = \"Negative\"\n","            elif senti == '1':\n","                sent_value = \"Positive\"\n","            else:\n","                sent_value = \"Neutral\"\n","            narrative['expression'][(narrative.abstraction == abst) & (narrative.cl_num == i)] = sent_value\n","        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"],"name":"stderr"},{"output_type":"stream","text":["['1', '2']\n","['1', '2']\n","['1']\n","['1']\n","['1']\n","['1']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['1']\n","['1']\n","['1']\n","['1']\n","['1']\n","['1']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n","['2']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XwlA6NcWRZ_u","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGu1V34_RZ_v","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGMWS30hRZ_z","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"553MFgHGRZ_0","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bZG4THm3RZ_2","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EWudO6zPRZ_4","colab_type":"text"},"source":["\n","\n","# Save the narratives in excel file\n"," With each sheet in the file representing 1 narrative ( == 1 cluster)"]},{"cell_type":"code","metadata":{"id":"-_Ac4MLiRZ_5","colab_type":"code","colab":{}},"source":["#sudo pip install xlwt\n","#sudo pip3 install openpyxl\n","from pandas import ExcelWriter\n","\n","#Save the narratives in an excel file \n","\n","writer = pd.ExcelWriter('narrative.xlsx')\n","for i in cluster_name:\n","    df1 = pd.DataFrame(dfUnique[['tweet','freq']][dfUnique.cl_num == i]).sort_values(['freq'], ascending = [0])\n","    df1 = pd.DataFrame({'tweet': dfUnique['tweet'][dfUnique.cl_num == i], 'freq': dfUnique['freq'][dfUnique.cl_num == i]}) \n","    df1 = df1.sort_values(['freq'], ascending = [0]) \n","\n","    df2 = pd.DataFrame({ 'abstraction': narrative['abstraction'][narrative.cl_num == i], 'expression': narrative['expression'][narrative.cl_num == i]})\n","    df3 = pd.DataFrame({'abstraction': (len(df1)-len(df2))*['-'], 'expression': (len(df1)-len(df2))*['-']})\n","    df2 = df2.append(df3)\n","\n","    df1 = df1.reset_index(drop=True)\n","    df2 = df2.reset_index(drop=True)\n","    df1['abstraction'] = df2['abstraction']\n","    df1['expression'] = df2['expression']\n","\n","    df1.to_excel(writer,'narrative_cluster'+str(i))\n","\n","writer.save()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6jm-3eERZ_6","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"vhh1K7wHRZ_9","colab_type":"code","colab":{},"outputId":"4fdcc6ae-e661-4984-c719-62b3ab7127c7"},"source":["narrative"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abstraction</th>\n","      <th>cl_num</th>\n","      <th>expression</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>support demonetization</td>\n","      <td>1.0</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nation 8086 people</td>\n","      <td>1.0</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>modibharosa huge support</td>\n","      <td>1.0</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>demonetization issue</td>\n","      <td>2.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>thedarjchron demonetization northeast reaction</td>\n","      <td>2.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>system india article</td>\n","      <td>3.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>overall impact</td>\n","      <td>3.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>shop owner gets</td>\n","      <td>3.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>politics nitishkumar supports</td>\n","      <td>5.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>coop banks</td>\n","      <td>5.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>question was paytm informed</td>\n","      <td>7.0</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>rssurjewala critical question</td>\n","      <td>7.0</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>incindia rssurjewala</td>\n","      <td>7.0</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>demonetization dea secy dasshaktikanta</td>\n","      <td>8.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>govt organisations</td>\n","      <td>8.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>aap join</td>\n","      <td>10.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>samajwadi partys jaya bachchan</td>\n","      <td>10.0</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>claimed benefit</td>\n","      <td>10.0</td>\n","      <td>Negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                       abstraction  cl_num expression\n","0                           support demonetization     1.0    Neutral\n","1                               nation 8086 people     1.0   Positive\n","2                         modibharosa huge support     1.0   Positive\n","3                             demonetization issue     2.0   Negative\n","4   thedarjchron demonetization northeast reaction     2.0   Negative\n","5                             system india article     3.0   Negative\n","6                                   overall impact     3.0   Negative\n","7                                  shop owner gets     3.0   Negative\n","8                    politics nitishkumar supports     5.0   Negative\n","9                                       coop banks     5.0   Negative\n","10                     question was paytm informed     7.0   Positive\n","11                   rssurjewala critical question     7.0   Positive\n","12                            incindia rssurjewala     7.0   Positive\n","13          demonetization dea secy dasshaktikanta     8.0   Negative\n","14                              govt organisations     8.0   Negative\n","15                                        aap join    10.0   Negative\n","16                  samajwadi partys jaya bachchan    10.0   Negative\n","17                                 claimed benefit    10.0   Negative"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"WzviXHxYRZ__","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4eQyq0MMRaAA","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfsCKt5zRaAC","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4W9kp5juRaAE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUfRyApbRaAG","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Guux7ukmRaAH","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nY18MPrTRaAJ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eP-DDBpoRaAK","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyuHD1e4RaAM","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apBsBC1pRaAQ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ogqgp7MoRaAS","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFzpy3MdRaAU","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6rXcf42LRaAW","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjPC4wceRaAY","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcAEyGBnRaAa","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9WShIkuMRaAc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T6cEiTEeRaAf","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B1cxEOVjRaAh","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zuWPsccwRaAj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hzTbNOBuRaAl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kq0s4KMNRaAo","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NgpHTf5XSB_Y","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gh6qM7c_i-4q","colab_type":"text"},"source":["### TODO:\n","\n","* Implement models.ldamulticore â€“ parallelized Latent Dirichlet Allocation using all CPU cores to parallelize and speed up model training.\n","* Switch from BERT/RoBERTa to SciBERT, BART, and or other models. "]}]}